{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Mobilenetv2 + SSD.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbsreEfjnQtP",
    "colab_type": "text"
   },
   "source": [
    "# Implementation of MobileNetv2+SSD<br>\n",
    "This is an implementation of the MobileNetv2 + SSD architecture for a relatively simpler task of determining bounding boxes for MNIST images embedded in a box. Each box contains only one digit(28x28 MNIST embedded into a 224x224 box) as of now, but the number of predictions per image can be expanded easily (the training outputs need to modified). Also, no data augmentation has been used till now (Colab kept crashing when I increased the dataset size beyond 1000, so the initial amount of data present was sufficient. The crashes might have been due to high traffic, but I haven't confirmed it).<p>\n",
    "In the earlier implementation, the ground truth data contained information about only one bounding box, which meant only one prediction per image ( reference https://colab.research.google.com/github/rs9899/mySSDimplementation/blob/master/MobileNetSSD_v2.ipynb#scrollTo=xWBzDsvkDqx5). For me, it also reduced the training signal and the model was overfitting. So I changed the outputs to a prediction for each default box (as it should be, from what I understood from the SSD paper). Although the initial implementation is good for the purposes for understanding the model.\n",
    "\n",
    "Comments mentioned throughout the code mention what needs to change if the model inputs or outputs are changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdL2AL8yAi00",
    "colab_type": "text"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_hvSY3X-AeFP",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "1039d725-6ddc-4032-f23c-a33ccd056bc8"
   },
   "source": [
    "import os\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy.matlib\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "# import Bottleneck"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFmrlFJ8uKCe",
    "colab_type": "text"
   },
   "source": [
    "Define Bottleneck Residual layer for MobileNet<br>\n",
    "Using the same parameters as mentioned in the paper"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xmwhyyS7CFyK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class Bottleneck(tf.keras.Model):\n",
    "  def __init__(self, expansion, filters, stride, block_id, alpha=1, ):\n",
    "    super(Bottleneck,self).__init__(name=\"Bottleneck_\"+block_id)\n",
    "    self.expansion = expansion\n",
    "    self.alpha = alpha\n",
    "    self.output_channels = self.alpha * filters\n",
    "    self.stride = stride\n",
    "    self.out = None # there was some problem with the eager execution\n",
    "\n",
    "    prefix =  'Bottleneck_{}_'.format(block_id)\n",
    "    self.prefix = prefix\n",
    "    # expansion\n",
    "    self.expand_BN = layers.BatchNormalization(name=prefix+'expand_BN')\n",
    "    self.expand_ReLU = layers.ReLU(max_value=6, name=prefix+'expand_ReLU')\n",
    "\n",
    "    #conv\n",
    "    self.Conv = layers.DepthwiseConv2D(kernel_size=3, padding='same', strides=self.stride, use_bias=False, name=prefix+'conv')\n",
    "    self.Conv_BN = layers.BatchNormalization(name=prefix+'conv_BN')\n",
    "    self.Conv_ReLU = layers.ReLU(max_value=6, name=prefix+'conv_ReLU')\n",
    "\n",
    "    #project\n",
    "    self.project = layers.Conv2D(filters=self.output_channels, kernel_size=1, use_bias=False, name='contract')\n",
    "    self.project_BN = layers.BatchNormalization(name=prefix+'contract_BN')\n",
    "\n",
    "    # dimensions need to be the same for residual connection\n",
    "    self.residual = layers.Add(name=prefix + 'residual')\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.d = input_shape[-1]\n",
    "    self.expand = layers.Conv2D(filters=self.expansion*self.d, kernel_size=1, use_bias=False, name=self.prefix+'expand')\n",
    "      \n",
    "  def call(self, inputs):\n",
    "    x = self.expand(inputs)\n",
    "    x = self.expand_BN(x)\n",
    "    x = self.expand_ReLU(x)\n",
    "    self.out = x\n",
    "    \n",
    "    x = self.Conv(x)\n",
    "    x = self.Conv_BN(x)\n",
    "    x = self.Conv_ReLU(x)\n",
    "\n",
    "    x = self.project(x)\n",
    "    x = self.project_BN(x)\n",
    "\n",
    "    if self.output_channels==self.d and self.stride==1:\n",
    "      x = self.residual([inputs, x])\n",
    "\n",
    "    return x\n",
    "\n",
    "  def model(self):\n",
    "      x = tf.keras.Input(shape=(28,28,3))\n",
    "      return tf.keras.Model(inputs=[x], outputs=self.call(x))"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VYGagWE8T2Et",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class MobileNetv2(tf.keras.Model):\n",
    "  def __init__(self, k=11):\n",
    "    super(MobileNetv2,self).__init__()\n",
    "    self.k = k\n",
    "\n",
    "    self.pad = layers.ZeroPadding2D(padding=2, name='pad')\n",
    "    self.conv_inp = layers.Conv2D(filters=32, kernel_size=3, strides=(2,2), padding='valid', use_bias=False, name='conv')\n",
    "    self.BN = layers.BatchNormalization(name='BN')\n",
    "    self.ReLU = layers.ReLU(max_value=6, name='ReLU')\n",
    "    \n",
    "    self.B1_1 = Bottleneck(expansion=1, filters=16, stride=1, block_id='B1_1')\n",
    "\n",
    "    self.B2_1 = Bottleneck(expansion=6, filters=24, stride=2, block_id='B2_1')\n",
    "    self.B2_2 = Bottleneck(expansion=6, filters=24, stride=1, block_id='B2_2')\n",
    "\n",
    "    self.B3_1 = Bottleneck(expansion=6, filters=32, stride=2, block_id='B3_1')\n",
    "    self.B3_2 = Bottleneck(expansion=6, filters=32, stride=1, block_id='B3_2')\n",
    "    self.B3_3 = Bottleneck(expansion=6, filters=32, stride=1, block_id='B3_3')\n",
    "\n",
    "    self.B4_1 = Bottleneck(expansion=6, filters=64, stride=2, block_id='B4_1')\n",
    "    self.B4_2 = Bottleneck(expansion=6, filters=64, stride=1, block_id='B4_2')\n",
    "    self.B4_3 = Bottleneck(expansion=6, filters=64, stride=1, block_id='B4_3')\n",
    "    self.B4_4 = Bottleneck(expansion=6, filters=64, stride=1, block_id='B4_4')\n",
    "\n",
    "    self.B5_1 = Bottleneck(expansion=6, filters=96, stride=1, block_id='B5_1')\n",
    "    self.B5_2 = Bottleneck(expansion=6, filters=96, stride=1, block_id='B5_2')\n",
    "    self.B5_3 = Bottleneck(expansion=6, filters=96, stride=1, block_id='B5_3')\n",
    "\n",
    "    self.B6_1 = Bottleneck(expansion=6, filters=160, stride=2, block_id='B6_1')\n",
    "    self.B6_2 = Bottleneck(expansion=6, filters=160, stride=1, block_id='B6_2')\n",
    "    self.B6_3 = Bottleneck(expansion=6, filters=160, stride=1, block_id='B6_3')\n",
    "\n",
    "    self.B7_1 = Bottleneck(expansion=6, filters=320, stride=1, block_id='B7_1')\n",
    "\n",
    "    self.conv_out = layers.Conv2D(filters=1280, kernel_size=1, strides=(1,1), use_bias=False, name='conv_out')\n",
    "    self.avgpool = layers.AveragePooling2D(pool_size=(7,7), name='avg_pool')\n",
    "    self.conv_seg = layers.Conv2D(filters=self.k, kernel_size=1, strides=(1,1), use_bias=False, name='conv_seg')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.conv_inp(inputs)\n",
    "    x = self.BN(x)\n",
    "    x = self.ReLU(x)\n",
    "\n",
    "    x = self.B1_1(x)\n",
    "\n",
    "    x = self.B2_1(x)\n",
    "    x = self.B2_2(x)\n",
    "\n",
    "    x = self.B3_1(x)\n",
    "    x = self.B3_2(x)\n",
    "    x = self.B3_3(x)\n",
    "    \n",
    "    x = self.B4_1(x)\n",
    "    x = self.B4_2(x)\n",
    "    x = self.B4_3(x)\n",
    "    x = self.B4_4(x)\n",
    "    \n",
    "    x = self.B5_1(x)\n",
    "    x = self.B5_2(x)\n",
    "    x = self.B5_3(x)\n",
    "    \n",
    "    x = self.B6_1(x)\n",
    "    x = self.B6_2(x)\n",
    "    x = self.B6_3(x)\n",
    "    \n",
    "    x = self.B7_1(x)\n",
    "\n",
    "    x = self.conv_out(x)\n",
    "    x = self.avgpool(x)\n",
    "    c4 = self.conv_seg(x)\n",
    "\n",
    "    return c4\n",
    "\n",
    "  def model(self):\n",
    "      x = tf.keras.Input(shape=(224,224,3))\n",
    "\n",
    "      return tf.keras.Model(inputs=x, outputs=self.call(x))"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "05i7363EYwmP",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "outputId": "d8434d3b-e85a-4481-b5bd-15751d6e9140"
   },
   "source": [
    "MobileNetv2().model().summary()"
   ],
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv (Conv2D)                (None, 111, 111, 32)      864       \n",
      "_________________________________________________________________\n",
      "BN (BatchNormalization)      (None, 111, 111, 32)      128       \n",
      "_________________________________________________________________\n",
      "ReLU (ReLU)                  (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "Bottleneck_B1_1 (Bottleneck) (None, 111, 111, 16)      2144      \n",
      "_________________________________________________________________\n",
      "Bottleneck_B2_1 (Bottleneck) (None, 56, 56, 24)        5568      \n",
      "_________________________________________________________________\n",
      "Bottleneck_B2_2 (Bottleneck) (None, 56, 56, 24)        9456      \n",
      "_________________________________________________________________\n",
      "Bottleneck_B3_1 (Bottleneck) (None, 28, 28, 32)        10640     \n",
      "_________________________________________________________________\n",
      "Bottleneck_B3_2 (Bottleneck) (None, 28, 28, 32)        15680     \n",
      "_________________________________________________________________\n",
      "Bottleneck_B3_3 (Bottleneck) (None, 28, 28, 32)        15680     \n",
      "_________________________________________________________________\n",
      "Bottleneck_B4_1 (Bottleneck) (None, 14, 14, 64)        21952     \n",
      "_________________________________________________________________\n",
      "Bottleneck_B4_2 (Bottleneck) (None, 14, 14, 64)        55936     \n",
      "_________________________________________________________________\n",
      "Bottleneck_B4_3 (Bottleneck) (None, 14, 14, 64)        55936     \n",
      "_________________________________________________________________\n",
      "Bottleneck_B4_4 (Bottleneck) (None, 14, 14, 64)        55936     \n",
      "_________________________________________________________________\n",
      "Bottleneck_B5_1 (Bottleneck) (None, 14, 14, 96)        68352     \n",
      "_________________________________________________________________\n",
      "Bottleneck_B5_2 (Bottleneck) (None, 14, 14, 96)        120768    \n",
      "_________________________________________________________________\n",
      "Bottleneck_B5_3 (Bottleneck) (None, 14, 14, 96)        120768    \n",
      "_________________________________________________________________\n",
      "Bottleneck_B6_1 (Bottleneck) (None, 7, 7, 160)         157888    \n",
      "_________________________________________________________________\n",
      "Bottleneck_B6_2 (Bottleneck) (None, 7, 7, 160)         324160    \n",
      "_________________________________________________________________\n",
      "Bottleneck_B6_3 (Bottleneck) (None, 7, 7, 160)         324160    \n",
      "_________________________________________________________________\n",
      "Bottleneck_B7_1 (Bottleneck) (None, 7, 7, 320)         478400    \n",
      "_________________________________________________________________\n",
      "conv_out (Conv2D)            (None, 7, 7, 1280)        409600    \n",
      "_________________________________________________________________\n",
      "avg_pool (AveragePooling2D)  (None, 1, 1, 1280)        0         \n",
      "_________________________________________________________________\n",
      "conv_seg (Conv2D)            (None, 1, 1, 11)          14080     \n",
      "=================================================================\n",
      "Total params: 2,268,096\n",
      "Trainable params: 2,236,480\n",
      "Non-trainable params: 31,616\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II03ZRu17FnE",
    "colab_type": "text"
   },
   "source": [
    "## SSD\n",
    "The default number of boxes per layer and resolution of each layer is different, since we are working with MNIST data and 224x224 image sizes.\n",
    "To change the number of boxes per layer and layerWidths, some constraints need to be kept in mind which are mentioned in the later sections"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aYD2gfR9O8L0",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class SSD(tf.keras.Model):\n",
    "  def __init__(self, numBoxes=[4,6,6,6,4,4], layerWidth=[28,14,7,4,2,1], k=10+1+4):\n",
    "    super(SSD,self).__init__()\n",
    "    self.classes = k\n",
    "    self.featureMaps = 6\n",
    "    self.MobileNet = MobileNetv2(k=k)\n",
    "    self.numBoxes = numBoxes\n",
    "    self.layerWidth = layerWidth\n",
    "    self.features = [None for _ in range(self.featureMaps)]\n",
    "    self.classifiers = [None for _ in range(self.featureMaps)]\n",
    "\n",
    "    self.conv1_1 = layers.Conv2D(256, 1, name='SSD_conv_1_1')\n",
    "    self.conv1_2 = layers.Conv2D(512, 3, strides=(2,2), padding='same', name='SSD_conv_1_2')\n",
    "\n",
    "    self.conv2_1 = layers.Conv2D(128, 1, name='SSD_conv_2_1')\n",
    "    self.conv2_2 = layers.Conv2D(256, 3, strides=(2,2), padding='same', name='SSD_conv_2_2')\n",
    "\n",
    "    self.conv3_1 = layers.Conv2D(128, 1, name='SSD_conv_3_1')\n",
    "    self.conv3_2 = layers.Conv2D(256, 3, strides=(1,1), name='SSD_conv_3_2')\n",
    "\n",
    "    self.conv4_1 = layers.Conv2D(128, 1, name='SSD_conv_4_1')\n",
    "    self.conv4_2 = layers.Conv2D(256, 2, strides=(1,1), name='SSD_conv_4_2') # changed the kernel size to 2 since the output of the previous layer has width 3\n",
    "    self.conv = []\n",
    "    self.reshape = []\n",
    "\n",
    "    for i in range(self.featureMaps):\n",
    "      self.conv.append(layers.Conv2D(self.numBoxes[i]*self.classes, 3, padding='same', name='Classification_'+str(i)))\n",
    "      self.reshape.append(layers.Reshape((self.layerWidth[i]*self.layerWidth[i]*self.numBoxes[i], self.classes),name='Reshape_classification_'+str(i)))\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.MobileNet.build(input_shape)\n",
    "\n",
    "  def call(self,inputs):\n",
    "    x = inputs\n",
    "    x = self.MobileNet(x)\n",
    "\n",
    "    # get the convolved images at different resolutions\n",
    "    self.features[0] = self.MobileNet.get_layer('Bottleneck_B4_1').out\n",
    "    self.features[1] = self.MobileNet.get_layer('Bottleneck_B5_3').out\n",
    "    self.features[2] = self.conv1_2(self.conv1_1(self.features[1]))\n",
    "    self.features[3] = self.conv2_2(self.conv2_1(self.features[2]))\n",
    "    self.features[4] = self.conv3_2(self.conv3_1(self.features[3]))\n",
    "    self.features[5] = self.conv4_2(self.conv4_1(self.features[4]))\n",
    "\n",
    "    for i in range(self.featureMaps):\n",
    "      # for each feature map, create predictions according to the number of boxes for that layer and the number of output channels\n",
    "      x = self.conv[i](self.features[i])\n",
    "      x = self.reshape[i](x)\n",
    "      self.classifiers[i] = x\n",
    "\n",
    "    # concatenate all the classifiers\n",
    "    x = layers.concatenate(self.classifiers, axis = -2, name='concatenate')\n",
    "    return x\n",
    "\n",
    "  def model(self):\n",
    "    x = tf.keras.Input(shape=(224,224,3))\n",
    "    return tf.keras.Model(inputs=x, outputs=self.call(x))"
   ],
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "29A_FW-GxK4t",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "NUM_CLASSES = 10\n",
    "# the first 2 dimensions should be equal to width of the output from the bottleneck expand ReLU at the (4,1) and (5,3) respectively.\n",
    "# the dimensions after the second one are determined by the convolutions written inside the SSD (conv1_2, conv2_2, conv3_3, conv4_2)\n",
    "layerWidths = [28,14,7,4,2,1]\n",
    "numBoxes = [3,3,3,3,3,3]\n",
    "assert len(numBoxes) == len(layerWidths) # numBoxes for each layer and each layer has a specific width\n",
    "outputChannels = NUM_CLASSES + 1 + 4 # 10 classes + background + cx,cy,h,w\n",
    "assert outputChannels - NUM_CLASSES == 5"
   ],
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FmOfPVIjCkuP",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "5fdfe35e-e5d8-4f1f-c112-da87335a2f74"
   },
   "source": [
    "model = SSD(numBoxes=numBoxes, layerWidth=layerWidths, k=outputChannels)\n",
    "model.model().summary()"
   ],
   "execution_count": 61,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Originated from a graph execution error.\n\nThe graph execution error is detected at a node built at (most recent call last):\n>>>  File /usr/lib/python3.8/runpy.py, line 194, in _run_module_as_main\n>>>  File /usr/lib/python3.8/runpy.py, line 87, in _run_code\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel_launcher.py, line 16, in <module>\n>>>  File /home/z/.local/lib/python3.8/site-packages/traitlets/config/application.py, line 846, in launch_instance\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py, line 677, in start\n>>>  File /home/z/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py, line 199, in start\n>>>  File /usr/lib/python3.8/asyncio/base_events.py, line 570, in run_forever\n>>>  File /usr/lib/python3.8/asyncio/base_events.py, line 1859, in _run_once\n>>>  File /usr/lib/python3.8/asyncio/events.py, line 81, in _run\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 457, in dispatch_queue\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 446, in process_one\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 353, in dispatch_shell\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 648, in execute_request\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py, line 353, in do_execute\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py, line 533, in run_cell\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 2914, in run_cell\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 2960, in _run_cell\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py, line 78, in _pseudo_sync_runner\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 3185, in run_cell_async\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 3377, in run_ast_nodes\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 3457, in run_code\n>>>  File /tmp/ipykernel_135703/3485494895.py, line 2, in <module>\n>>>  File /tmp/ipykernel_135703/483238357.py, line 57, in model\n>>>  File /tmp/ipykernel_135703/483238357.py, line 35, in call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py, line 983, in __call__\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py, line 1121, in _functional_construction_call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py, line 854, in _keras_tensor_symbolic_call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py, line 894, in _infer_output_signature\n>>>  File /tmp/ipykernel_135703/917028808.py, line 61, in call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py, line 1044, in __call__\n>>>  File /tmp/ipykernel_135703/4255659242.py, line 35, in call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py, line 1044, in __call__\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/advanced_activations.py, line 433, in call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py, line 150, in error_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py, line 1096, in op_dispatch_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/backend.py, line 4745, in relu\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py, line 150, in error_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py, line 1096, in op_dispatch_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py, line 3634, in relu6\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py, line 10616, in relu6\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py, line 744, in _apply_op_helper\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py, line 689, in _create_op_internal\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py, line 3697, in _create_op_internal\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py, line 2101, in __init__\n\nError detected in node 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6' defined at: File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\", line 4745, in relu\n\nTypeError: tf.Graph captured an external symbolic tensor. The symbolic tensor 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6:0' created by node 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6' is captured by the tf.Graph being executed as an input. But a tf.Graph is not allowed to take symbolic tensors from another graph as its inputs. Make sure all captured inputs of the executing tf.Graph are not symbolic tensors. Use return values, explicit Python locals or TensorFlow collections to access it. Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_135703/3485494895.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSSD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnumBoxes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnumBoxes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlayerWidth\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlayerWidths\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutputChannels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msummary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_135703/483238357.py\u001B[0m in \u001B[0;36mmodel\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m     \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInput\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m224\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m224\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 57\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_135703/483238357.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMobileNet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Bottleneck_B4_1'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMobileNet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Bottleneck_B5_3'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv1_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv1_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv2_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv2_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv3_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv3_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1042\u001B[0m         with autocast_variable.enable_auto_cast_variables(\n\u001B[1;32m   1043\u001B[0m             self._compute_dtype_object):\n\u001B[0;32m-> 1044\u001B[0;31m           \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1045\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1046\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_activity_regularizer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    251\u001B[0m       \u001B[0minputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compute_causal_padding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 253\u001B[0;31m     \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_convolution_op\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkernel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    254\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    255\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muse_bias\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 153\u001B[0;31m       \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    154\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    155\u001B[0m       \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     56\u001B[0m   \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[1;32m     59\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[1;32m     60\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Originated from a graph execution error.\n\nThe graph execution error is detected at a node built at (most recent call last):\n>>>  File /usr/lib/python3.8/runpy.py, line 194, in _run_module_as_main\n>>>  File /usr/lib/python3.8/runpy.py, line 87, in _run_code\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel_launcher.py, line 16, in <module>\n>>>  File /home/z/.local/lib/python3.8/site-packages/traitlets/config/application.py, line 846, in launch_instance\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py, line 677, in start\n>>>  File /home/z/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py, line 199, in start\n>>>  File /usr/lib/python3.8/asyncio/base_events.py, line 570, in run_forever\n>>>  File /usr/lib/python3.8/asyncio/base_events.py, line 1859, in _run_once\n>>>  File /usr/lib/python3.8/asyncio/events.py, line 81, in _run\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 457, in dispatch_queue\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 446, in process_one\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 353, in dispatch_shell\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 648, in execute_request\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py, line 353, in do_execute\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py, line 533, in run_cell\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 2914, in run_cell\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 2960, in _run_cell\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py, line 78, in _pseudo_sync_runner\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 3185, in run_cell_async\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 3377, in run_ast_nodes\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 3457, in run_code\n>>>  File /tmp/ipykernel_135703/3485494895.py, line 2, in <module>\n>>>  File /tmp/ipykernel_135703/483238357.py, line 57, in model\n>>>  File /tmp/ipykernel_135703/483238357.py, line 35, in call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py, line 983, in __call__\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py, line 1121, in _functional_construction_call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py, line 854, in _keras_tensor_symbolic_call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py, line 894, in _infer_output_signature\n>>>  File /tmp/ipykernel_135703/917028808.py, line 61, in call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py, line 1044, in __call__\n>>>  File /tmp/ipykernel_135703/4255659242.py, line 35, in call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py, line 1044, in __call__\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/advanced_activations.py, line 433, in call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py, line 150, in error_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py, line 1096, in op_dispatch_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/backend.py, line 4745, in relu\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py, line 150, in error_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py, line 1096, in op_dispatch_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py, line 3634, in relu6\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py, line 10616, in relu6\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py, line 744, in _apply_op_helper\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py, line 689, in _create_op_internal\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py, line 3697, in _create_op_internal\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py, line 2101, in __init__\n\nError detected in node 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6' defined at: File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\", line 4745, in relu\n\nTypeError: tf.Graph captured an external symbolic tensor. The symbolic tensor 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6:0' created by node 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6' is captured by the tf.Graph being executed as an input. But a tf.Graph is not allowed to take symbolic tensors from another graph as its inputs. Make sure all captured inputs of the executing tf.Graph are not symbolic tensors. Use return values, explicit Python locals or TensorFlow collections to access it. Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_135703/3035046171.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msummary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36msummary\u001B[0;34m(self, line_length, positions, print_fn)\u001B[0m\n\u001B[1;32m   2507\u001B[0m     \"\"\"\n\u001B[1;32m   2508\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilt\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2509\u001B[0;31m       raise ValueError('This model has not yet been built. '\n\u001B[0m\u001B[1;32m   2510\u001B[0m                        \u001B[0;34m'Build the model first by calling `build()` or calling '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2511\u001B[0m                        \u001B[0;34m'`fit()` with some data, or specify '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RA7NxfQ7_G6",
    "colab_type": "text"
   },
   "source": [
    "## Creating boxes and IoU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yRYi7Ez7UzpH",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# I have used less varying custom scales and aspect ratios here, since the dataset is already uniform\n",
    "#IMPORTANT: before changing the scales and aspect ratios, read the comment below\n",
    "\n",
    "# number of scales is equal to the number of different resolutions ie num of layer widths\n",
    "# for a given resolution, we have different aspect ratios\n",
    "# num(scales) = num(layerWidth) = num(numBoxes) and num(asp_ratios) = numBoxes[i]\n",
    "MinScale = .1 # Min and Max scale given as percentage\n",
    "MaxScale = 1.5\n",
    "scales = [ MinScale + x/len(layerWidths) * (MaxScale-MinScale) for x in range(len(layerWidths)) ]\n",
    "scales = scales[::-1] # reversing the order because the layerWidths go from high to low (lower to higher resoltuion)\n",
    "\n",
    "asp = [0.5,1.0,1.5]\n",
    "asp1 = [x**0.5 for x in asp]\n",
    "asp2 = [1/x for x in asp1]\n",
    "IMG_SIZE = 224\n",
    "# should be equal to the 1st dimension in the output layer of the SSD model\n",
    "BOXES = sum([a*a*b for a,b in zip(layerWidths,numBoxes)])\n",
    "centres = np.zeros((BOXES,2))\n",
    "hw = np.zeros((BOXES,2))\n",
    "boxes = np.zeros((BOXES,4))\n",
    "print(BOXES)"
   ],
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3150\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9A1xGMrXVX18",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# calculating the default box centres and height, width\n",
    "idx = 0\n",
    "\n",
    "for gridSize, numBox, scale in zip(layerWidths,numBoxes,scales):\n",
    "  step_size = IMG_SIZE*1.0/gridSize\n",
    "  for i in range(gridSize):\n",
    "    for j in range(gridSize):\n",
    "      pos = idx + (i*gridSize+j) * numBox\n",
    "      # centre is the same for all aspect ratios(=numBox)\n",
    "      centres[ pos : pos + numBox , :] = i*step_size + step_size/2, j*step_size + step_size/2\n",
    "      # height and width vary according to the scale and aspect ratio\n",
    "      # zip asepct ratios and then scale them by the scaling factor\n",
    "      hw[ pos : pos + numBox , :] = np.multiply(gridSize*scale, np.squeeze(np.dstack([asp1,asp2]),axis=0))[:numBox,:]\n",
    "\n",
    "  idx += gridSize*gridSize*numBox\n",
    "\n",
    "# (x,y) co-ordinates of top left and bottom right\n",
    "# This actually is not used anywhere. centres[] and hw[] are a good enough substitute\n",
    "boxes[:,0] = centres[:,0] - hw[:,0]/2\n",
    "boxes[:,1] = centres[:,1] - hw[:,1]/2\n",
    "boxes[:,2] = centres[:,0] + hw[:,0]/2\n",
    "boxes[:,3] = centres[:,1] + hw[:,1]/2"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TJSIPHPMh3N2",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# calculate IoU for a set of search boxes and default boxes\n",
    "def IoU(box1, box2):\n",
    "  box1 = box1.astype(np.float64)\n",
    "  box2 = box2.astype(np.float64)\n",
    "  # find the left and right co-ordinates of the edges. Min should be less than Max for non zero overlap\n",
    "  xmin = np.maximum(box1[:,0],box2[:,0])\n",
    "  xmax = np.minimum(box1[:,2],box2[:,2])\n",
    "  ymin = np.maximum(box1[:,1],box2[:,1])\n",
    "  ymax = np.minimum(box1[:,3],box2[:,3])\n",
    "\n",
    "  intersection = np.abs(np.maximum(xmax-xmin,0) * np.maximum(ymax-ymin,0))\n",
    "  boxArea1 = np.abs((box1[:,2] - box1[:,0]) * (box1[:,3] - box1[:,1]))\n",
    "  boxArea2 = np.abs((box2[:,2] - box2[:,0]) * (box2[:,3] - box2[:,1]))\n",
    "  unionArea = boxArea1 + boxArea2 - intersection\n",
    "  assert (unionArea > 0).all()\n",
    "  iou = intersection / unionArea\n",
    "\n",
    "  return iou\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "# give the index of the box correpsonding to the IoUs > threshold (=0.5)\n",
    "def bestIoU(searchBox):\n",
    "  return np.argwhere(IoU(numpy.matlib.repmat(searchBox,BOXES,1), boxes) > THRESHOLD)"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm2k4c5Ik_BX",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "TRAINSIZE = 600\n",
    "TESTSIZE = 100\n",
    "\n",
    "x_train = x_train[:TRAINSIZE, : , :]\n",
    "y_train = y_train[:TRAINSIZE]\n",
    "x_test = x_test[:TESTSIZE, : , :]\n",
    "y_test = y_test[:TESTSIZE]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert data for the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PEgx1Sdcq7yJ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# take mnist x and y pairs and convert to input, output pairs for the MobileNetv2+SSD model\n",
    "def convert(x,y):\n",
    "  MNIST_SIZE = x.shape[-1]\n",
    "  # create a 2D array of top left corners for the mnist image to be placed\n",
    "  corner = np.random.randint(IMG_SIZE - MNIST_SIZE, size=(x.shape[0],2))\n",
    "\n",
    "  # create a blank canvas for the input with the required dimension\n",
    "  input = np.zeros((x.shape[0], IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "  # replacing a part by RGB version of MNIST\n",
    "  for i in range(x.shape[0]):\n",
    "    lx = int(corner[i,0])\n",
    "    ly = int(corner[i,1])\n",
    "    input[i,lx:lx + MNIST_SIZE, ly:ly+MNIST_SIZE,:] = np.repeat(np.expand_dims(np.array(x[i,:,:]),axis=-1),3,axis=-1)\n",
    "\n",
    "  # for each default box, there are 5 values: class number and delta cx,cy,h,w\n",
    "  output = np.zeros((y.shape[0],BOXES,1+4))\n",
    "  output[:,:,0] = NUM_CLASSES # defaulting class labels for all boxes to background initially\n",
    "  for i in range(x.shape[0]):\n",
    "    bbox = np.zeros(4)\n",
    "    bbox[:2] = corner[i]\n",
    "    bbox[2:] = corner[i] + (MNIST_SIZE,MNIST_SIZE)\n",
    "    # for all default boxes which have IoU > threshold, set the delta values and class number\n",
    "    box_idx = bestIoU(bbox).astype(np.uint16)\n",
    "    output[i,box_idx,0] = y[i]\n",
    "    output[i,box_idx,1] = (bbox[0] + bbox[2])/2.0 - centres[box_idx,0]\n",
    "    output[i,box_idx,2] = (bbox[1] + bbox[3])/2.0 - centres[box_idx,1]\n",
    "    output[i,box_idx,3] = MNIST_SIZE - hw[box_idx,0]\n",
    "    output[i,box_idx,4] = MNIST_SIZE - hw[box_idx,1]\n",
    "\n",
    "  return input, output\n"
   ],
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Sk_z17wV3Bj5",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "test_x, test_y = convert(x_test,y_test)\n",
    "train_x, train_y = convert(x_train,y_train)"
   ],
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NAwnJnu4qE0P",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "outputId": "27116f08-73e6-42e7-c8d8-daac27654afc"
   },
   "source": [
    "# checking if the inputs prepared are correct or not\n",
    "r = np.random.randint(0,train_x.shape[0])\n",
    "img = train_x[r,:,:,:].copy()\n",
    "img_y = train_y[r]\n",
    "\n",
    "im = np.array(Image.fromarray(img.astype(np.uint8)))\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow(im)\n",
    "\n",
    "# find all boxes where class label is not background\n",
    "idx = np.argwhere(img_y[:,0] != NUM_CLASSES)[:,0]\n",
    "print('Number of boxes with IoU > 0.5:',idx.shape[0])\n",
    "print('Green box: ground truth. Red box: default boxes with IoU > threshold')\n",
    "\n",
    "#calculating the ground truth bounding boxes\n",
    "gt = np.zeros(4,dtype=np.uint16)\n",
    "gt[:2] = (img_y[idx[0],1:3] + centres[idx[0],:2])\n",
    "gt[2:] = (img_y[idx[0],3:] + hw[idx[0],:])\n",
    "\n",
    "# for some reason, x and y are inverted\n",
    "rect = patches.Rectangle((gt[1]-gt[3]/2,gt[0]-gt[2]/2),gt[3],gt[2],linewidth=5,edgecolor='g',facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "\n",
    "# showing all the boxes with IoU > 0.5\n",
    "for i in idx:\n",
    "  rect = patches.Rectangle((centres[i][1]-hw[i,1]/2, centres[i][0]-hw[i,0]/2), hw[i,1], hw[i,0], linewidth=1, edgecolor='r', facecolor='none')\n",
    "  ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of boxes with IoU > 0.5: 8\n",
      "Green box: ground truth. Red box: default boxes with IoU > threshold\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1nI7mXjS8zA9",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "88b50f5a-8931-4f30-eb69-f211e964d690"
   },
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "\n",
    "print(train_dataset.element_spec)\n",
    "print(test_dataset.element_spec)"
   ],
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(224, 224, 3), dtype=tf.float64, name=None), TensorSpec(shape=(3150, 5), dtype=tf.float64, name=None))\n",
      "(TensorSpec(shape=(224, 224, 3), dtype=tf.float64, name=None), TensorSpec(shape=(3150, 5), dtype=tf.float64, name=None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 16:18:22.977210: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 722534400 exceeds 10% of free system memory.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oyX8dnwQ8_1k",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "BATCH_SIZE = 10\n",
    "SHUFFLE_BUFFER_SIZE = 60\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LOSS FUNCTION\n",
    "Hard negative mining hasn't been done here\n",
    "Initial idea was to assign weights to background classes, but there is some problem in that approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# label is not required here in the standard implementation\n",
    "# calculate the smooth L1 loss\n",
    "def smoothL1(x,y,label):\n",
    "  diff = K.abs(x-y) #* K.switch(label == 10, label*1.0/BOXES, label)\n",
    "  result = K.switch(diff < 1, 0.5 * diff**2, diff - 0.5)\n",
    "\n",
    "  return K.mean(result)\n",
    "\n",
    "\n",
    "def confidenceLoss(y,label):\n",
    "  unweighted_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(label, y)\n",
    "  # class_weights = tf.constant([[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0/BOXES]]*BOXES])\n",
    "  # weights = tf.reduce_sum(class_weights * y, axis = -1)\n",
    "  # weighted_loss = unweighted_loss * weights\n",
    "\n",
    "  return K.mean(unweighted_loss)\n",
    "\n",
    "\n",
    "def Loss(gt,y):\n",
    "  # shape of y is n * BOXES * output_channels\n",
    "  # shape of gt is n * BOXES * 5 \n",
    "  loss = 0\n",
    "  # localisation loss\n",
    "  loss += smoothL1(y[:,:,-4:],gt[:,:,-4:],gt[:,:,0:1])\n",
    "  # confidence loss\n",
    "  loss += confidenceLoss(y[:,:,:-4],tf.cast(gt[:,:,0],tf.int32))\n",
    "\n",
    "  return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TRAIN MODEL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ssd_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobile_netv2_12 (MobileNetv2 (None, 1, 1, 15)          2273216   \n",
      "_________________________________________________________________\n",
      "SSD_conv_1_1 (Conv2D)        multiple                  147712    \n",
      "_________________________________________________________________\n",
      "SSD_conv_1_2 (Conv2D)        multiple                  1180160   \n",
      "_________________________________________________________________\n",
      "SSD_conv_2_1 (Conv2D)        multiple                  65664     \n",
      "_________________________________________________________________\n",
      "SSD_conv_2_2 (Conv2D)        multiple                  295168    \n",
      "_________________________________________________________________\n",
      "SSD_conv_3_1 (Conv2D)        multiple                  32896     \n",
      "_________________________________________________________________\n",
      "SSD_conv_3_2 (Conv2D)        multiple                  295168    \n",
      "_________________________________________________________________\n",
      "SSD_conv_4_1 (Conv2D)        multiple                  32896     \n",
      "_________________________________________________________________\n",
      "SSD_conv_4_2 (Conv2D)        multiple                  131328    \n",
      "_________________________________________________________________\n",
      "Classification_0 (Conv2D)    multiple                  77805     \n",
      "_________________________________________________________________\n",
      "Classification_1 (Conv2D)    multiple                  233325    \n",
      "_________________________________________________________________\n",
      "Classification_2 (Conv2D)    multiple                  207405    \n",
      "_________________________________________________________________\n",
      "Classification_3 (Conv2D)    multiple                  103725    \n",
      "_________________________________________________________________\n",
      "Classification_4 (Conv2D)    multiple                  103725    \n",
      "_________________________________________________________________\n",
      "Classification_5 (Conv2D)    multiple                  103725    \n",
      "_________________________________________________________________\n",
      "Reshape_classification_0 (Re multiple                  0         \n",
      "_________________________________________________________________\n",
      "Reshape_classification_1 (Re multiple                  0         \n",
      "_________________________________________________________________\n",
      "Reshape_classification_2 (Re multiple                  0         \n",
      "_________________________________________________________________\n",
      "Reshape_classification_3 (Re multiple                  0         \n",
      "_________________________________________________________________\n",
      "Reshape_classification_4 (Re multiple                  0         \n",
      "_________________________________________________________________\n",
      "Reshape_classification_5 (Re multiple                  0         \n",
      "=================================================================\n",
      "Total params: 5,283,918\n",
      "Trainable params: 5,252,302\n",
      "Non-trainable params: 31,616\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A8_O3V8DB_Gk",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001), loss=Loss)\n",
    "history = model.fit(train_dataset, epochs=2, validation_data=test_dataset)"
   ],
   "execution_count": 65,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 17:20:16.912007: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 722534400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_conv/depthwise_kernel:0', 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_conv_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_conv_BN/beta:0', 'mobile_netv2_12/Bottleneck_B5_3/contract/kernel:0', 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_contract_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_contract_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_expand_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_expand_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_conv/depthwise_kernel:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_conv_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_conv_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_1/contract/kernel:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_contract_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_contract_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_expand/kernel:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_expand_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_expand_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_conv/depthwise_kernel:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_conv_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_conv_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_2/contract/kernel:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_contract_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_contract_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_expand/kernel:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_expand_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_expand_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_conv/depthwise_kernel:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_conv_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_conv_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_3/contract/kernel:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_contract_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_contract_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_expand/kernel:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_expand_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_expand_BN/beta:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_conv/depthwise_kernel:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_conv_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_conv_BN/beta:0', 'mobile_netv2_12/Bottleneck_B7_1/contract/kernel:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_contract_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_contract_BN/beta:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_expand/kernel:0', 'mobile_netv2_12/conv_out/kernel:0', 'mobile_netv2_12/conv_seg/kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_conv/depthwise_kernel:0', 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_conv_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_conv_BN/beta:0', 'mobile_netv2_12/Bottleneck_B5_3/contract/kernel:0', 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_contract_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B5_3/Bottleneck_B5_3_contract_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_expand_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_expand_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_conv/depthwise_kernel:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_conv_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_conv_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_1/contract/kernel:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_contract_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_contract_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_1/Bottleneck_B6_1_expand/kernel:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_expand_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_expand_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_conv/depthwise_kernel:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_conv_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_conv_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_2/contract/kernel:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_contract_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_contract_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_2/Bottleneck_B6_2_expand/kernel:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_expand_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_expand_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_conv/depthwise_kernel:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_conv_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_conv_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_3/contract/kernel:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_contract_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_contract_BN/beta:0', 'mobile_netv2_12/Bottleneck_B6_3/Bottleneck_B6_3_expand/kernel:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_expand_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_expand_BN/beta:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_conv/depthwise_kernel:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_conv_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_conv_BN/beta:0', 'mobile_netv2_12/Bottleneck_B7_1/contract/kernel:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_contract_BN/gamma:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_contract_BN/beta:0', 'mobile_netv2_12/Bottleneck_B7_1/Bottleneck_B7_1_expand/kernel:0', 'mobile_netv2_12/conv_out/kernel:0', 'mobile_netv2_12/conv_seg/kernel:0'] when minimizing the loss.\n",
      "60/60 [==============================] - 29s 416ms/step - loss: 0.3698 - val_loss: 0.3427\n",
      "Epoch 2/2\n",
      "60/60 [==============================] - 25s 411ms/step - loss: 0.0975 - val_loss: 0.0516\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.convolutional.ZeroPadding2D object at 0x7f782f8e6340>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.merge.Add object at 0x7f782ee907f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.merge.Add object at 0x7f782eb1ebb0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.merge.Add object at 0x7f782e4edf40>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.merge.Add object at 0x7f782f9b7fd0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.merge.Add object at 0x7f782d77f790>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.merge.Add object at 0x7f782d476df0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.merge.Add object at 0x7f7950224b80>, because it is not built.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to save function b'__inference_ssd_8_layer_call_and_return_conditional_losses_123920' because it captures graph tensor Tensor(\"Bottleneck_B5_3_expand_ReLU/PartitionedCall:0\", shape=(None, 14, 14, 576), dtype=float32) from a parent function which cannot be converted to a constant with `tf.get_static_value`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_135703/1359712065.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m## save model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mmodel_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"pplcntr_\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mdt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrftime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"%Y%m%d-%H%M%S\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m model.save(\n\u001B[0m\u001B[1;32m      4\u001B[0m   \u001B[0mfilepath\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmodel_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m   \u001B[0moverwrite\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36msave\u001B[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001B[0m\n\u001B[1;32m   2132\u001B[0m     \"\"\"\n\u001B[1;32m   2133\u001B[0m     \u001B[0;31m# pylint: enable=line-too-long\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2134\u001B[0;31m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001B[0m\u001B[1;32m   2135\u001B[0m                     signatures, options, save_traces)\n\u001B[1;32m   2136\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001B[0m in \u001B[0;36msave_model\u001B[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001B[0m\n\u001B[1;32m    148\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mgeneric_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSharedObjectSavingScope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001B[0m\u001B[1;32m    151\u001B[0m                             signatures, options, save_traces)\n\u001B[1;32m    152\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save.py\u001B[0m in \u001B[0;36msave\u001B[0;34m(model, filepath, overwrite, include_optimizer, signatures, options, save_traces)\u001B[0m\n\u001B[1;32m     87\u001B[0m   \u001B[0;32mwith\u001B[0m \u001B[0mK\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdeprecated_internal_learning_phase_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     88\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras_option_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msave_traces\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 89\u001B[0;31m       saved_nodes, node_paths = save_lib.save_and_return_nodes(\n\u001B[0m\u001B[1;32m     90\u001B[0m           model, filepath, signatures, options)\n\u001B[1;32m     91\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\u001B[0m in \u001B[0;36msave_and_return_nodes\u001B[0;34m(obj, export_dir, signatures, options, experimental_skip_checkpoint)\u001B[0m\n\u001B[1;32m   1313\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1314\u001B[0m   _, exported_graph, object_saver, asset_info, saved_nodes, node_paths = (\n\u001B[0;32m-> 1315\u001B[0;31m       _build_meta_graph(obj, signatures, options, meta_graph_def))\n\u001B[0m\u001B[1;32m   1316\u001B[0m   saved_model.saved_model_schema_version = (\n\u001B[1;32m   1317\u001B[0m       constants.SAVED_MODEL_SCHEMA_VERSION)\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\u001B[0m in \u001B[0;36m_build_meta_graph\u001B[0;34m(obj, signatures, options, meta_graph_def)\u001B[0m\n\u001B[1;32m   1485\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1486\u001B[0m   \u001B[0;32mwith\u001B[0m \u001B[0msave_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1487\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_build_meta_graph_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignatures\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmeta_graph_def\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\u001B[0m in \u001B[0;36m_build_meta_graph_impl\u001B[0;34m(obj, signatures, options, meta_graph_def)\u001B[0m\n\u001B[1;32m   1437\u001B[0m                                 wrapped_functions)\n\u001B[1;32m   1438\u001B[0m   \u001B[0mobject_saver\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mutil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTrackableSaver\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheckpoint_graph_view\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1439\u001B[0;31m   asset_info, exported_graph = _fill_meta_graph_def(\n\u001B[0m\u001B[1;32m   1440\u001B[0m       \u001B[0mmeta_graph_def\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msaveable_view\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignatures\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1441\u001B[0m       options.namespace_whitelist, options.experimental_custom_gradients)\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\u001B[0m in \u001B[0;36m_fill_meta_graph_def\u001B[0;34m(meta_graph_def, saveable_view, signature_functions, namespace_whitelist, save_custom_gradients)\u001B[0m\n\u001B[1;32m    896\u001B[0m   \u001B[0mresource_initializer_ops\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    897\u001B[0m   \u001B[0;32mwith\u001B[0m \u001B[0mexported_graph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_default\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 898\u001B[0;31m     \u001B[0mobject_map\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresource_map\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0masset_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msaveable_view\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap_resources\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    899\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mresource_initializer_function\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mresource_initializer_functions\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    900\u001B[0m       \u001B[0masset_dependencies\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\u001B[0m in \u001B[0;36mmap_resources\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    400\u001B[0m           \u001B[0mcapture_constant_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtensor_util\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconstant_value\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcapture\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    401\u001B[0m           \u001B[0;32mif\u001B[0m \u001B[0mcapture_constant_value\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 402\u001B[0;31m             raise ValueError(\n\u001B[0m\u001B[1;32m    403\u001B[0m                 \u001B[0;34mf\"Unable to save function {concrete_function.name} because it \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    404\u001B[0m                 \u001B[0;34mf\"captures graph tensor {capture} from a parent function which \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Unable to save function b'__inference_ssd_8_layer_call_and_return_conditional_losses_123920' because it captures graph tensor Tensor(\"Bottleneck_B5_3_expand_ReLU/PartitionedCall:0\", shape=(None, 14, 14, 576), dtype=float32) from a parent function which cannot be converted to a constant with `tf.get_static_value`."
     ]
    }
   ],
   "source": [
    "## save model\n",
    "model_name = \"pplcntr_\" + dt.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model.save(\n",
    "  filepath=model_name,\n",
    "  overwrite=True,\n",
    "  include_optimizer=True,\n",
    "  save_format='tf',\n",
    "  signatures=None,\n",
    "  options=None,\n",
    "  save_traces=True,)\n",
    "# model.save(model_name, save_format='tf')\n",
    "\n",
    "# ## save weight\n",
    "# model_weight = \"pplcntr_\" + dt.now().strftime(\"%Y%m%d-%H%M%S\") + \".h5\"\n",
    "# model.save_weights(model_weight, save_format='h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g_n2VfMsg1NT",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "e20fc8d4-968c-4272-9c62-57eeefcee1b3"
   },
   "source": [
    "model.evaluate(test_x, test_y)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oNuY-45SngR",
    "colab_type": "text"
   },
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VTfYjsyJTEtv",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# create some sample data\n",
    "X, Y = convert(x_test, y_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QPazH1zFTnE4",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "535d81f5-7835-47ce-98a0-6d78bfe13c78"
   },
   "source": [
    "# get prediction for one sample\n",
    "y_pred = model.predict(X)\n",
    "print(y_pred.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jrD03dgjcZMO",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "OBJperCLASS = 10 # get the top 10 results for each class\n",
    "# get the confidence scores (with class values) and delta for the boxes. For each class, the top 10 values are used\n",
    "def infer(Y):\n",
    "  # classes are actually the index into the default boxes\n",
    "  classes = np.zeros((OBJperCLASS,outputChannels-4),dtype=np.uint16)\n",
    "  conf = np.zeros((OBJperCLASS,outputChannels-4))\n",
    "  delta = np.zeros((OBJperCLASS,outputChannels-4,4))\n",
    "  class_predictions = softmax(Y[:,:outputChannels-4],axis=1)\n",
    "  for i in range(outputChannels-4):\n",
    "    classes[:,i] = Bottleneck.argpartition(class_predictions[:,i],BOXES-1-10,axis=-1)[-OBJperCLASS:]\n",
    "    conf[:,i] = class_predictions[classes[:,i],i]\n",
    "    delta[:,i] = Y[classes[:,i],outputChannels-4:]\n",
    "\n",
    "  return conf,classes, delta\n",
    "\n",
    "\n",
    "# generate bounding boxes from the inferred outputs\n",
    "def Bbox(confidence,box_idx,delta):\n",
    "  #delta contains delta(cx,cy,h,w)\n",
    "  bbox_centre = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
    "  bbox_hw = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
    "  for i in range(OBJperCLASS):\n",
    "    bbox_centre[i,:,0] = centres[box_idx[i]][:,0]+delta[i,:,0]\n",
    "    bbox_centre[i,:,1] = centres[box_idx[i]][:,1]+delta[i,:,1]\n",
    "    bbox_hw[i,:,0] = hw[box_idx[i]][:,0] + delta[i,:,2]\n",
    "    bbox_hw[i,:,1] = hw[box_idx[i]][:,1]+delta[i,:,3]\n",
    "\n",
    "  return bbox_centre,bbox_hw"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LY7SOlpafX51",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "outputId": "eef5e296-27f6-46f1-bfeb-422573f1d1e1"
   },
   "source": [
    "r = np.random.randint(TESTSIZE)\n",
    "\n",
    "# top 10 predictions for each class\n",
    "confidence, box_idx, delta = infer(y_pred[r])\n",
    "bbox_centre,bbox_hw = Bbox(confidence, box_idx, delta)\n",
    "\n",
    "im = np.array(Image.fromarray(X[r].astype(np.uint8)))\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow(im)\n",
    "\n",
    "for i in range(outputChannels-4):\n",
    "  # skipping backgrounds\n",
    "  if i == NUM_CLASSES:\n",
    "    continue\n",
    "  color = 'r'\n",
    "  # if a class is mentioned in the ground truth, color the boxes green\n",
    "  if i in Y[r,:,0]:\n",
    "    color = 'g'\n",
    "    print(i)\n",
    "  \n",
    "  # skip all the classes which have low confidence values\n",
    "  if (confidence[:,i] > 0.5).any() or i in Y[r,:,0]:\n",
    "    for k in range(OBJperCLASS):\n",
    "      print(\"{}: Confidence-{}\\t\\tCentre-{} Height,Width-{}\".format(i,confidence[k,i],bbox_centre[k,i],bbox_hw[k,i]))\n",
    "      \n",
    "      # draw bounding box only if confidence scores are high\n",
    "      if confidence[k,i] < 0.5:\n",
    "        continue\n",
    "      x = bbox_centre[k,i,0] - bbox_hw[k,i,0]/2\n",
    "      y = bbox_centre[k,i,1] - bbox_hw[k,i,1]/2\n",
    "      rect = patches.Rectangle((y,x),bbox_hw[k,i,1],bbox_hw[k,i,0],linewidth=1,edgecolor=color,facecolor='none')\n",
    "      ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}