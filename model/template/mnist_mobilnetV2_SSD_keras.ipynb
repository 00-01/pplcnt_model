{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Mobilenetv2 + SSD.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbsreEfjnQtP",
    "colab_type": "text"
   },
   "source": [
    "# Implementation of MobileNetv2+SSD<br>\n",
    "This is an implementation of the MobileNetv2 + SSD architecture for a relatively simpler task of determining bounding boxes for MNIST images embedded in a box. Each box contains only one digit(28x28 MNIST embedded into a 224x224 box) as of now, but the number of predictions per image can be expanded easily (the training outputs need to modified). Also, no data augmentation has been used till now (Colab kept crashing when I increased the dataset size beyond 1000, so the initial amount of data present was sufficient. The crashes might have been due to high traffic, but I haven't confirmed it).<p>\n",
    "In the earlier implementation, the ground truth data contained information about only one bounding box, which meant only one prediction per image ( reference https://colab.research.google.com/github/rs9899/mySSDimplementation/blob/master/MobileNetSSD_v2.ipynb#scrollTo=xWBzDsvkDqx5). For me, it also reduced the training signal and the model was overfitting. So I changed the outputs to a prediction for each default box (as it should be, from what I understood from the SSD paper). Although the initial implementation is good for the purposes for understanding the model.\n",
    "\n",
    "Comments mentioned throughout the code mention what needs to change if the model inputs or outputs are changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdL2AL8yAi00",
    "colab_type": "text"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_hvSY3X-AeFP",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "1039d725-6ddc-4032-f23c-a33ccd056bc8"
   },
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy.matlib\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "# import Bottleneck"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 19:09:32.134996: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/z/GWT/gap_sdk/install/workstation/lib\n",
      "2022-02-15 19:09:32.135016: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFmrlFJ8uKCe",
    "colab_type": "text"
   },
   "source": [
    "Define Bottleneck Residual layer for MobileNet<br>\n",
    "Using the same parameters as mentioned in the paper"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xmwhyyS7CFyK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class Bottleneck(tf.keras.Model):\n",
    "  def __init__(self, expansion, filters, stride, block_id, alpha=1, ):\n",
    "    super(Bottleneck, self).__init__(name=\"Bottleneck_\" + block_id)\n",
    "    self.expansion = expansion\n",
    "    self.alpha = alpha\n",
    "    self.output_channels = self.alpha * filters\n",
    "    self.stride = stride\n",
    "    self.out = None  # there was some problem with the eager execution\n",
    "\n",
    "    prefix = 'Bottleneck_{}_'.format(block_id)\n",
    "    self.prefix = prefix\n",
    "    # expansion\n",
    "    self.expand_BN = layers.BatchNormalization(name=prefix + 'expand_BN')\n",
    "    self.expand_ReLU = layers.ReLU(max_value=6, name=prefix + 'expand_ReLU')\n",
    "\n",
    "    #conv\n",
    "    self.Conv = layers.DepthwiseConv2D(kernel_size=3, padding='same', strides=self.stride, use_bias=False, name=prefix + 'conv')\n",
    "    self.Conv_BN = layers.BatchNormalization(name=prefix + 'conv_BN')\n",
    "    self.Conv_ReLU = layers.ReLU(max_value=6, name=prefix + 'conv_ReLU')\n",
    "\n",
    "    #project\n",
    "    self.project = layers.Conv2D(filters=self.output_channels, kernel_size=1, use_bias=False, name='contract')\n",
    "    self.project_BN = layers.BatchNormalization(name=prefix + 'contract_BN')\n",
    "\n",
    "    # dimensions need to be the same for residual connection\n",
    "    self.residual = layers.Add(name=prefix + 'residual')\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.d = input_shape[-1]\n",
    "    self.expand = layers.Conv2D(filters=self.expansion * self.d, kernel_size=1, use_bias=False, name=self.prefix + 'expand')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.expand(inputs)\n",
    "    x = self.expand_BN(x)\n",
    "    x = self.expand_ReLU(x)\n",
    "    self.out = x\n",
    "\n",
    "    x = self.Conv(x)\n",
    "    x = self.Conv_BN(x)\n",
    "    x = self.Conv_ReLU(x)\n",
    "\n",
    "    x = self.project(x)\n",
    "    x = self.project_BN(x)\n",
    "\n",
    "    if self.output_channels == self.d and self.stride == 1:\n",
    "      x = self.residual([inputs, x])\n",
    "\n",
    "    return x\n",
    "\n",
    "  def model(self):\n",
    "    x = tf.keras.Input(shape=(28, 28, 3))\n",
    "    return tf.keras.Model(inputs=[x], outputs=self.call(x))"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VYGagWE8T2Et",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class MobileNetv2(tf.keras.Model):\n",
    "  def __init__(self, k=11):\n",
    "    super(MobileNetv2, self).__init__()\n",
    "    self.k = k\n",
    "\n",
    "    self.pad = layers.ZeroPadding2D(padding=2, name='pad')\n",
    "    self.conv_inp = layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), padding='valid', use_bias=False, name='conv')\n",
    "    self.BN = layers.BatchNormalization(name='BN')\n",
    "    self.ReLU = layers.ReLU(max_value=6, name='ReLU')\n",
    "\n",
    "    self.B1_1 = Bottleneck(expansion=1, filters=16, stride=1, block_id='B1_1')\n",
    "\n",
    "    self.B2_1 = Bottleneck(expansion=6, filters=24, stride=2, block_id='B2_1')\n",
    "    self.B2_2 = Bottleneck(expansion=6, filters=24, stride=1, block_id='B2_2')\n",
    "\n",
    "    self.B3_1 = Bottleneck(expansion=6, filters=32, stride=2, block_id='B3_1')\n",
    "    self.B3_2 = Bottleneck(expansion=6, filters=32, stride=1, block_id='B3_2')\n",
    "    self.B3_3 = Bottleneck(expansion=6, filters=32, stride=1, block_id='B3_3')\n",
    "\n",
    "    self.B4_1 = Bottleneck(expansion=6, filters=64, stride=2, block_id='B4_1')\n",
    "    self.B4_2 = Bottleneck(expansion=6, filters=64, stride=1, block_id='B4_2')\n",
    "    self.B4_3 = Bottleneck(expansion=6, filters=64, stride=1, block_id='B4_3')\n",
    "    self.B4_4 = Bottleneck(expansion=6, filters=64, stride=1, block_id='B4_4')\n",
    "\n",
    "    self.B5_1 = Bottleneck(expansion=6, filters=96, stride=1, block_id='B5_1')\n",
    "    self.B5_2 = Bottleneck(expansion=6, filters=96, stride=1, block_id='B5_2')\n",
    "    self.B5_3 = Bottleneck(expansion=6, filters=96, stride=1, block_id='B5_3')\n",
    "\n",
    "    self.B6_1 = Bottleneck(expansion=6, filters=160, stride=2, block_id='B6_1')\n",
    "    self.B6_2 = Bottleneck(expansion=6, filters=160, stride=1, block_id='B6_2')\n",
    "    self.B6_3 = Bottleneck(expansion=6, filters=160, stride=1, block_id='B6_3')\n",
    "\n",
    "    self.B7_1 = Bottleneck(expansion=6, filters=320, stride=1, block_id='B7_1')\n",
    "\n",
    "    self.conv_out = layers.Conv2D(filters=1280, kernel_size=1, strides=(1, 1), use_bias=False, name='conv_out')\n",
    "    self.avgpool = layers.AveragePooling2D(pool_size=(7, 7), name='avg_pool')\n",
    "    self.conv_seg = layers.Conv2D(filters=self.k, kernel_size=1, strides=(1, 1), use_bias=False, name='conv_seg')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.conv_inp(inputs)\n",
    "    x = self.BN(x)\n",
    "    x = self.ReLU(x)\n",
    "\n",
    "    x = self.B1_1(x)\n",
    "\n",
    "    x = self.B2_1(x)\n",
    "    x = self.B2_2(x)\n",
    "\n",
    "    x = self.B3_1(x)\n",
    "    x = self.B3_2(x)\n",
    "    x = self.B3_3(x)\n",
    "\n",
    "    x = self.B4_1(x)\n",
    "    x = self.B4_2(x)\n",
    "    x = self.B4_3(x)\n",
    "    x = self.B4_4(x)\n",
    "\n",
    "    x = self.B5_1(x)\n",
    "    x = self.B5_2(x)\n",
    "    x = self.B5_3(x)\n",
    "\n",
    "    x = self.B6_1(x)\n",
    "    x = self.B6_2(x)\n",
    "    x = self.B6_3(x)\n",
    "\n",
    "    x = self.B7_1(x)\n",
    "\n",
    "    x = self.conv_out(x)\n",
    "    x = self.avgpool(x)\n",
    "    c4 = self.conv_seg(x)\n",
    "\n",
    "    return c4\n",
    "\n",
    "  def model(self):\n",
    "    x = tf.keras.Input(shape=(224, 224, 3))\n",
    "    return tf.keras.Model(inputs=x, outputs=self.call(x))"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "05i7363EYwmP",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "outputId": "d8434d3b-e85a-4481-b5bd-15751d6e9140"
   },
   "source": [
    "MobileNetv2().model().summary()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 19:09:33.455770: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-02-15 19:09:33.455794: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: z\n",
      "2022-02-15 19:09:33.455799: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: z\n",
      "2022-02-15 19:09:33.455846: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.47.3\n",
      "2022-02-15 19:09:33.455861: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.47.3\n",
      "2022-02-15 19:09:33.455866: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.47.3\n",
      "2022-02-15 19:09:33.456177: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv (Conv2D)               (None, 111, 111, 32)      864       \n",
      "                                                                 \n",
      " BN (BatchNormalization)     (None, 111, 111, 32)      128       \n",
      "                                                                 \n",
      " ReLU (ReLU)                 (None, 111, 111, 32)      0         \n",
      "                                                                 \n",
      " Bottleneck_B1_1 (Bottleneck  (None, 111, 111, 16)     2144      \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B2_1 (Bottleneck  (None, 56, 56, 24)       5568      \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B2_2 (Bottleneck  (None, 56, 56, 24)       9456      \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B3_1 (Bottleneck  (None, 28, 28, 32)       10640     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B3_2 (Bottleneck  (None, 28, 28, 32)       15680     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B3_3 (Bottleneck  (None, 28, 28, 32)       15680     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B4_1 (Bottleneck  (None, 14, 14, 64)       21952     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B4_2 (Bottleneck  (None, 14, 14, 64)       55936     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B4_3 (Bottleneck  (None, 14, 14, 64)       55936     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B4_4 (Bottleneck  (None, 14, 14, 64)       55936     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B5_1 (Bottleneck  (None, 14, 14, 96)       68352     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B5_2 (Bottleneck  (None, 14, 14, 96)       120768    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B5_3 (Bottleneck  (None, 14, 14, 96)       120768    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B6_1 (Bottleneck  (None, 7, 7, 160)        157888    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B6_2 (Bottleneck  (None, 7, 7, 160)        324160    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B6_3 (Bottleneck  (None, 7, 7, 160)        324160    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B7_1 (Bottleneck  (None, 7, 7, 320)        478400    \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_out (Conv2D)           (None, 7, 7, 1280)        409600    \n",
      "                                                                 \n",
      " avg_pool (AveragePooling2D)  (None, 1, 1, 1280)       0         \n",
      "                                                                 \n",
      " conv_seg (Conv2D)           (None, 1, 1, 11)          14080     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,268,096\n",
      "Trainable params: 2,236,480\n",
      "Non-trainable params: 31,616\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II03ZRu17FnE",
    "colab_type": "text"
   },
   "source": [
    "## SSD\n",
    "The default number of boxes per layer and resolution of each layer is different, since we are working with MNIST data and 224x224 image sizes.\n",
    "To change the number of boxes per layer and layerWidths, some constraints need to be kept in mind which are mentioned in the later sections"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aYD2gfR9O8L0",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class SSD(tf.keras.Model):\n",
    "  def __init__(self, numBoxes=[4,6,6,6,4,4], layerWidth=[28,14,7,4,2,1], k=10+1+4):\n",
    "    super(SSD,self).__init__()\n",
    "    self.classes = k\n",
    "    self.featureMaps = 6\n",
    "    self.MobileNet = MobileNetv2(k=k)\n",
    "    self.numBoxes = numBoxes\n",
    "    self.layerWidth = layerWidth\n",
    "    self.features = [None for _ in range(self.featureMaps)]\n",
    "    self.classifiers = [None for _ in range(self.featureMaps)]\n",
    "\n",
    "    self.conv1_1 = layers.Conv2D(256, 1, name='SSD_conv_1_1')\n",
    "    self.conv1_2 = layers.Conv2D(512, 3, strides=(2,2), padding='same', name='SSD_conv_1_2')\n",
    "\n",
    "    self.conv2_1 = layers.Conv2D(128, 1, name='SSD_conv_2_1')\n",
    "    self.conv2_2 = layers.Conv2D(256, 3, strides=(2,2), padding='same', name='SSD_conv_2_2')\n",
    "\n",
    "    self.conv3_1 = layers.Conv2D(128, 1, name='SSD_conv_3_1')\n",
    "    self.conv3_2 = layers.Conv2D(256, 3, strides=(1,1), name='SSD_conv_3_2')\n",
    "\n",
    "    self.conv4_1 = layers.Conv2D(128, 1, name='SSD_conv_4_1')\n",
    "    self.conv4_2 = layers.Conv2D(256, 2, strides=(1,1), name='SSD_conv_4_2') # changed the kernel size to 2 since the output of the previous layer has width 3\n",
    "    self.conv = []\n",
    "    self.reshape = []\n",
    "\n",
    "    for i in range(self.featureMaps):\n",
    "      self.conv.append(layers.Conv2D(self.numBoxes[i]*self.classes, 3, padding='same', name='Classification_'+str(i)))\n",
    "      self.reshape.append(layers.Reshape((self.layerWidth[i]*self.layerWidth[i]*self.numBoxes[i], self.classes),name='Reshape_classification_'+str(i)))\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.MobileNet.build(input_shape)\n",
    "\n",
    "  def call(self,inputs):\n",
    "    x = inputs\n",
    "    x = self.MobileNet(x)\n",
    "\n",
    "    # get the convolved images at different resolutions\n",
    "    self.features[0] = self.MobileNet.get_layer('Bottleneck_B4_1').out\n",
    "    self.features[1] = self.MobileNet.get_layer('Bottleneck_B5_3').out\n",
    "    self.features[2] = self.conv1_2(self.conv1_1(self.features[1]))\n",
    "    self.features[3] = self.conv2_2(self.conv2_1(self.features[2]))\n",
    "    self.features[4] = self.conv3_2(self.conv3_1(self.features[3]))\n",
    "    self.features[5] = self.conv4_2(self.conv4_1(self.features[4]))\n",
    "\n",
    "    for i in range(self.featureMaps):\n",
    "      # for each feature map, create predictions according to the number of boxes for that layer and the number of output channels\n",
    "      x = self.conv[i](self.features[i])\n",
    "      x = self.reshape[i](x)\n",
    "      self.classifiers[i] = x\n",
    "\n",
    "    # concatenate all the classifiers\n",
    "    x = layers.concatenate(self.classifiers, axis=-2, name='concatenate')\n",
    "    return x\n",
    "\n",
    "  def model(self):\n",
    "    x = tf.keras.Input(shape=(224,224,3))\n",
    "    return tf.keras.Model(inputs=x, outputs=self.call(x))"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "29A_FW-GxK4t",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "NUM_CLASSES = 10\n",
    "# the first 2 dimensions should be equal to width of the output from the bottleneck expand ReLU at the (4,1) and (5,3) respectively.\n",
    "# the dimensions after the second one are determined by the convolutions written inside the SSD (conv1_2, conv2_2, conv3_3, conv4_2)\n",
    "layerWidths = [28,14,7,4,2,1]\n",
    "numBoxes = [3,3,3,3,3,3]\n",
    "assert len(numBoxes) == len(layerWidths) # numBoxes for each layer and each layer has a specific width\n",
    "outputChannels = NUM_CLASSES + 1 + 4 # 10 classes + background + cx,cy,h,w\n",
    "assert outputChannels - NUM_CLASSES == 5"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FmOfPVIjCkuP",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "5fdfe35e-e5d8-4f1f-c112-da87335a2f74"
   },
   "source": [
    "model = SSD(numBoxes=numBoxes, layerWidth=layerWidths, k=outputChannels)\n",
    "model.model().summary()"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"SSD_conv_1_1\" (type Conv2D).\n\nOriginated from a graph execution error.\n\nThe graph execution error is detected at a node built at (most recent call last):\n>>>  File /usr/lib/python3.8/runpy.py, line 194, in _run_module_as_main\n>>>  File /usr/lib/python3.8/runpy.py, line 87, in _run_code\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel_launcher.py, line 16, in <module>\n>>>  File /home/z/.local/lib/python3.8/site-packages/traitlets/config/application.py, line 846, in launch_instance\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py, line 677, in start\n>>>  File /home/z/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py, line 199, in start\n>>>  File /usr/lib/python3.8/asyncio/base_events.py, line 570, in run_forever\n>>>  File /usr/lib/python3.8/asyncio/base_events.py, line 1859, in _run_once\n>>>  File /usr/lib/python3.8/asyncio/events.py, line 81, in _run\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 457, in dispatch_queue\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 446, in process_one\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 353, in dispatch_shell\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 648, in execute_request\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py, line 353, in do_execute\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py, line 533, in run_cell\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 2914, in run_cell\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 2960, in _run_cell\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py, line 78, in _pseudo_sync_runner\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 3185, in run_cell_async\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 3377, in run_ast_nodes\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 3457, in run_code\n>>>  File /tmp/ipykernel_477301/3485494895.py, line 2, in <module>\n>>>  File /tmp/ipykernel_477301/483238357.py, line 57, in model\n>>>  File /tmp/ipykernel_477301/483238357.py, line 35, in call\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py, line 64, in error_handler\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py, line 1032, in __call__\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py, line 1173, in _functional_construction_call\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py, line 897, in _keras_tensor_symbolic_call\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py, line 942, in _infer_output_signature\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py, line 92, in error_handler\n>>>  File /tmp/ipykernel_477301/2663660010.py, line 60, in call\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py, line 64, in error_handler\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py, line 1096, in __call__\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py, line 92, in error_handler\n>>>  File /tmp/ipykernel_477301/333962661.py, line 35, in call\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py, line 64, in error_handler\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py, line 1096, in __call__\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py, line 92, in error_handler\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/layers/advanced_activations.py, line 433, in call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py, line 150, in error_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py, line 1096, in op_dispatch_handler\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/backend.py, line 4953, in relu\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py, line 150, in error_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py, line 1096, in op_dispatch_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py, line 3634, in relu6\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py, line 10616, in relu6\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py, line 744, in _apply_op_helper\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py, line 689, in _create_op_internal\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py, line 3697, in _create_op_internal\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py, line 2101, in __init__\n\nError detected in node 'mobile_netv2_2/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6' defined at: File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 4953, in relu\n\nTypeError: tf.Graph captured an external symbolic tensor. The symbolic tensor 'mobile_netv2_2/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6:0' created by node 'mobile_netv2_2/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6' is captured by the tf.Graph being executed as an input. But a tf.Graph is not allowed to take symbolic tensors from another graph as its inputs. Make sure all captured inputs of the executing tf.Graph are not symbolic tensors. Use return values, explicit Python locals or TensorFlow collections to access it. Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 14, 14, 576), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_477301/3485494895.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSSD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnumBoxes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnumBoxes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlayerWidth\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlayerWidths\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutputChannels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msummary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_477301/483238357.py\u001B[0m in \u001B[0;36mmodel\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m     \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInput\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m224\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m224\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 57\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_477301/483238357.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMobileNet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Bottleneck_B4_1'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMobileNet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Bottleneck_B5_3'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv1_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv1_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv2_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv2_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv3_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv3_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m       \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     68\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m       \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     56\u001B[0m   \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[1;32m     59\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[1;32m     60\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Exception encountered when calling layer \"SSD_conv_1_1\" (type Conv2D).\n\nOriginated from a graph execution error.\n\nThe graph execution error is detected at a node built at (most recent call last):\n>>>  File /usr/lib/python3.8/runpy.py, line 194, in _run_module_as_main\n>>>  File /usr/lib/python3.8/runpy.py, line 87, in _run_code\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel_launcher.py, line 16, in <module>\n>>>  File /home/z/.local/lib/python3.8/site-packages/traitlets/config/application.py, line 846, in launch_instance\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py, line 677, in start\n>>>  File /home/z/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py, line 199, in start\n>>>  File /usr/lib/python3.8/asyncio/base_events.py, line 570, in run_forever\n>>>  File /usr/lib/python3.8/asyncio/base_events.py, line 1859, in _run_once\n>>>  File /usr/lib/python3.8/asyncio/events.py, line 81, in _run\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 457, in dispatch_queue\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 446, in process_one\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 353, in dispatch_shell\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py, line 648, in execute_request\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py, line 353, in do_execute\n>>>  File /home/z/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py, line 533, in run_cell\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 2914, in run_cell\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 2960, in _run_cell\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py, line 78, in _pseudo_sync_runner\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 3185, in run_cell_async\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 3377, in run_ast_nodes\n>>>  File /home/z/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py, line 3457, in run_code\n>>>  File /tmp/ipykernel_477301/3485494895.py, line 2, in <module>\n>>>  File /tmp/ipykernel_477301/483238357.py, line 57, in model\n>>>  File /tmp/ipykernel_477301/483238357.py, line 35, in call\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py, line 64, in error_handler\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py, line 1032, in __call__\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py, line 1173, in _functional_construction_call\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py, line 897, in _keras_tensor_symbolic_call\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py, line 942, in _infer_output_signature\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py, line 92, in error_handler\n>>>  File /tmp/ipykernel_477301/2663660010.py, line 60, in call\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py, line 64, in error_handler\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py, line 1096, in __call__\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py, line 92, in error_handler\n>>>  File /tmp/ipykernel_477301/333962661.py, line 35, in call\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py, line 64, in error_handler\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py, line 1096, in __call__\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py, line 92, in error_handler\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/layers/advanced_activations.py, line 433, in call\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py, line 150, in error_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py, line 1096, in op_dispatch_handler\n>>>  File /usr/local/lib/python3.8/dist-packages/keras/backend.py, line 4953, in relu\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py, line 150, in error_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py, line 1096, in op_dispatch_handler\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py, line 3634, in relu6\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py, line 10616, in relu6\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py, line 744, in _apply_op_helper\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py, line 689, in _create_op_internal\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py, line 3697, in _create_op_internal\n>>>  File /home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py, line 2101, in __init__\n\nError detected in node 'mobile_netv2_2/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6' defined at: File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 4953, in relu\n\nTypeError: tf.Graph captured an external symbolic tensor. The symbolic tensor 'mobile_netv2_2/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6:0' created by node 'mobile_netv2_2/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6' is captured by the tf.Graph being executed as an input. But a tf.Graph is not allowed to take symbolic tensors from another graph as its inputs. Make sure all captured inputs of the executing tf.Graph are not symbolic tensors. Use return values, explicit Python locals or TensorFlow collections to access it. Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 14, 14, 576), dtype=float32)"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_477301/3035046171.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msummary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001B[0m in \u001B[0;36msummary\u001B[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable)\u001B[0m\n\u001B[1;32m   2773\u001B[0m     \"\"\"\n\u001B[1;32m   2774\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilt\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2775\u001B[0;31m       raise ValueError(\n\u001B[0m\u001B[1;32m   2776\u001B[0m           \u001B[0;34m'This model has not yet been built. '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2777\u001B[0m           \u001B[0;34m'Build the model first by calling `build()` or by calling '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RA7NxfQ7_G6",
    "colab_type": "text"
   },
   "source": [
    "## Creating boxes and IoU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yRYi7Ez7UzpH",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# I have used less varying custom scales and aspect ratios here, since the dataset is already uniform\n",
    "#IMPORTANT: before changing the scales and aspect ratios, read the comment below\n",
    "\n",
    "# number of scales is equal to the number of different resolutions ie num of layer widths\n",
    "# for a given resolution, we have different aspect ratios\n",
    "# num(scales) = num(layerWidth) = num(numBoxes) and num(asp_ratios) = numBoxes[i]\n",
    "MinScale = .1 # Min and Max scale given as percentage\n",
    "MaxScale = 1.5\n",
    "scales = [ MinScale + x/len(layerWidths) * (MaxScale-MinScale) for x in range(len(layerWidths)) ]\n",
    "scales = scales[::-1] # reversing the order because the layerWidths go from high to low (lower to higher resoltuion)\n",
    "\n",
    "asp = [0.5,1.0,1.5]\n",
    "asp1 = [x**0.5 for x in asp]\n",
    "asp2 = [1/x for x in asp1]\n",
    "IMG_SIZE = 224\n",
    "# should be equal to the 1st dimension in the output layer of the SSD model\n",
    "BOXES = sum([a*a*b for a,b in zip(layerWidths,numBoxes)])\n",
    "centres = np.zeros((BOXES,2))\n",
    "hw = np.zeros((BOXES,2))\n",
    "boxes = np.zeros((BOXES,4))\n",
    "print(BOXES)"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3150\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9A1xGMrXVX18",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# calculating the default box centres and height, width\n",
    "idx = 0\n",
    "\n",
    "for gridSize, numBox, scale in zip(layerWidths,numBoxes,scales):\n",
    "  step_size = IMG_SIZE*1.0/gridSize\n",
    "  for i in range(gridSize):\n",
    "    for j in range(gridSize):\n",
    "      pos = idx + (i*gridSize+j) * numBox\n",
    "      # centre is the same for all aspect ratios(=numBox)\n",
    "      centres[ pos : pos + numBox , :] = i*step_size + step_size/2, j*step_size + step_size/2\n",
    "      # height and width vary according to the scale and aspect ratio\n",
    "      # zip asepct ratios and then scale them by the scaling factor\n",
    "      hw[ pos : pos + numBox , :] = np.multiply(gridSize*scale, np.squeeze(np.dstack([asp1,asp2]),axis=0))[:numBox,:]\n",
    "\n",
    "  idx += gridSize*gridSize*numBox\n",
    "\n",
    "# (x,y) co-ordinates of top left and bottom right\n",
    "# This actually is not used anywhere. centres[] and hw[] are a good enough substitute\n",
    "boxes[:,0] = centres[:,0] - hw[:,0]/2\n",
    "boxes[:,1] = centres[:,1] - hw[:,1]/2\n",
    "boxes[:,2] = centres[:,0] + hw[:,0]/2\n",
    "boxes[:,3] = centres[:,1] + hw[:,1]/2"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TJSIPHPMh3N2",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# calculate IoU for a set of search boxes and default boxes\n",
    "def IoU(box1, box2):\n",
    "  box1 = box1.astype(np.float64)\n",
    "  box2 = box2.astype(np.float64)\n",
    "  # find the left and right co-ordinates of the edges. Min should be less than Max for non zero overlap\n",
    "  xmin = np.maximum(box1[:,0],box2[:,0])\n",
    "  xmax = np.minimum(box1[:,2],box2[:,2])\n",
    "  ymin = np.maximum(box1[:,1],box2[:,1])\n",
    "  ymax = np.minimum(box1[:,3],box2[:,3])\n",
    "\n",
    "  intersection = np.abs(np.maximum(xmax-xmin,0) * np.maximum(ymax-ymin,0))\n",
    "  boxArea1 = np.abs((box1[:,2] - box1[:,0]) * (box1[:,3] - box1[:,1]))\n",
    "  boxArea2 = np.abs((box2[:,2] - box2[:,0]) * (box2[:,3] - box2[:,1]))\n",
    "  unionArea = boxArea1 + boxArea2 - intersection\n",
    "  assert (unionArea > 0).all()\n",
    "  iou = intersection / unionArea\n",
    "\n",
    "  return iou\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "# give the index of the box correpsonding to the IoUs > threshold (=0.5)\n",
    "def bestIoU(searchBox):\n",
    "  return np.argwhere(IoU(numpy.matlib.repmat(searchBox,BOXES,1), boxes) > THRESHOLD)"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm2k4c5Ik_BX",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "TRAINSIZE = 600\n",
    "TESTSIZE = 100\n",
    "\n",
    "x_train = x_train[:TRAINSIZE, : , :]\n",
    "y_train = y_train[:TRAINSIZE]\n",
    "x_test = x_test[:TESTSIZE, : , :]\n",
    "y_test = y_test[:TESTSIZE]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert data for the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PEgx1Sdcq7yJ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# take mnist x and y pairs and convert to input, output pairs for the MobileNetv2+SSD model\n",
    "def convert(x,y):\n",
    "  MNIST_SIZE = x.shape[-1]\n",
    "  # create a 2D array of top left corners for the mnist image to be placed\n",
    "  corner = np.random.randint(IMG_SIZE - MNIST_SIZE, size=(x.shape[0],2))\n",
    "\n",
    "  # create a blank canvas for the input with the required dimension\n",
    "  input = np.zeros((x.shape[0], IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "  # replacing a part by RGB version of MNIST\n",
    "  for i in range(x.shape[0]):\n",
    "    lx = int(corner[i,0])\n",
    "    ly = int(corner[i,1])\n",
    "    input[i,lx:lx + MNIST_SIZE, ly:ly+MNIST_SIZE,:] = np.repeat(np.expand_dims(np.array(x[i,:,:]),axis=-1),3,axis=-1)\n",
    "\n",
    "  # for each default box, there are 5 values: class number and delta cx,cy,h,w\n",
    "  output = np.zeros((y.shape[0],BOXES,1+4))\n",
    "  output[:,:,0] = NUM_CLASSES # defaulting class labels for all boxes to background initially\n",
    "  for i in range(x.shape[0]):\n",
    "    bbox = np.zeros(4)\n",
    "    bbox[:2] = corner[i]\n",
    "    bbox[2:] = corner[i] + (MNIST_SIZE,MNIST_SIZE)\n",
    "    # for all default boxes which have IoU > threshold, set the delta values and class number\n",
    "    box_idx = bestIoU(bbox).astype(np.uint16)\n",
    "    output[i,box_idx,0] = y[i]\n",
    "    output[i,box_idx,1] = (bbox[0] + bbox[2])/2.0 - centres[box_idx,0]\n",
    "    output[i,box_idx,2] = (bbox[1] + bbox[3])/2.0 - centres[box_idx,1]\n",
    "    output[i,box_idx,3] = MNIST_SIZE - hw[box_idx,0]\n",
    "    output[i,box_idx,4] = MNIST_SIZE - hw[box_idx,1]\n",
    "\n",
    "  return input, output\n"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Sk_z17wV3Bj5",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "test_x, test_y = convert(x_test,y_test)\n",
    "train_x, train_y = convert(x_train,y_train)"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NAwnJnu4qE0P",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "outputId": "27116f08-73e6-42e7-c8d8-daac27654afc"
   },
   "source": [
    "# checking if the inputs prepared are correct or not\n",
    "r = np.random.randint(0,train_x.shape[0])\n",
    "img = train_x[r,:,:,:].copy()\n",
    "img_y = train_y[r]\n",
    "\n",
    "im = np.array(Image.fromarray(img.astype(np.uint8)))\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow(im)\n",
    "\n",
    "# find all boxes where class label is not background\n",
    "idx = np.argwhere(img_y[:,0] != NUM_CLASSES)[:,0]\n",
    "print('Number of boxes with IoU > 0.5:',idx.shape[0])\n",
    "print('Green box: ground truth. Red box: default boxes with IoU > threshold')\n",
    "\n",
    "#calculating the ground truth bounding boxes\n",
    "gt = np.zeros(4,dtype=np.uint16)\n",
    "gt[:2] = (img_y[idx[0],1:3] + centres[idx[0],:2])\n",
    "gt[2:] = (img_y[idx[0],3:] + hw[idx[0],:])\n",
    "\n",
    "# for some reason, x and y are inverted\n",
    "rect = patches.Rectangle((gt[1]-gt[3]/2,gt[0]-gt[2]/2),gt[3],gt[2],linewidth=5,edgecolor='g',facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "\n",
    "# showing all the boxes with IoU > 0.5\n",
    "for i in idx:\n",
    "  rect = patches.Rectangle((centres[i][1]-hw[i,1]/2, centres[i][0]-hw[i,0]/2), hw[i,1], hw[i,0], linewidth=1, edgecolor='r', facecolor='none')\n",
    "  ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of boxes with IoU > 0.5: 9\n",
      "Green box: ground truth. Red box: default boxes with IoU > threshold\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1nI7mXjS8zA9",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "88b50f5a-8931-4f30-eb69-f211e964d690"
   },
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "\n",
    "print(train_dataset.element_spec)\n",
    "print(test_dataset.element_spec)"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(224, 224, 3), dtype=tf.float64, name=None), TensorSpec(shape=(3150, 5), dtype=tf.float64, name=None))\n",
      "(TensorSpec(shape=(224, 224, 3), dtype=tf.float64, name=None), TensorSpec(shape=(3150, 5), dtype=tf.float64, name=None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 19:10:04.611873: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 722534400 exceeds 10% of free system memory.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oyX8dnwQ8_1k",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "BATCH_SIZE = 10\n",
    "SHUFFLE_BUFFER_SIZE = 60\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LOSS FUNCTION\n",
    "Hard negative mining hasn't been done here\n",
    "Initial idea was to assign weights to background classes, but there is some problem in that approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# label is not required here in the standard implementation\n",
    "# calculate the smooth L1 loss\n",
    "def smoothL1(x,y,label):\n",
    "  diff = K.abs(x-y) #* K.switch(label == 10, label*1.0/BOXES, label)\n",
    "  result = K.switch(diff < 1, 0.5 * diff**2, diff - 0.5)\n",
    "\n",
    "  return K.mean(result)\n",
    "\n",
    "\n",
    "def confidenceLoss(y,label):\n",
    "  unweighted_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(label, y)\n",
    "  # class_weights = tf.constant([[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0/BOXES]]*BOXES])\n",
    "  # weights = tf.reduce_sum(class_weights * y, axis = -1)\n",
    "  # weighted_loss = unweighted_loss * weights\n",
    "\n",
    "  return K.mean(unweighted_loss)\n",
    "\n",
    "\n",
    "def Loss(gt,y):\n",
    "  # shape of y is n * BOXES * output_channels\n",
    "  # shape of gt is n * BOXES * 5 \n",
    "  loss = 0\n",
    "  # localisation loss\n",
    "  loss += smoothL1(y[:,:,-4:],gt[:,:,-4:],gt[:,:,0:1])\n",
    "  # confidence loss\n",
    "  loss += confidenceLoss(y[:,:,:-4],tf.cast(gt[:,:,0],tf.int32))\n",
    "\n",
    "  return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TRAIN MODEL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A8_O3V8DB_Gk",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001), loss=Loss)\n",
    "history = model.fit(train_dataset, epochs=2, validation_data=test_dataset)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## save model\n",
    "model_name = \"pplcntr_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model.save(\n",
    "  filepath=model_name,\n",
    "  overwrite=True,\n",
    "  include_optimizer=True,\n",
    "  save_format='tf',\n",
    "  signatures=None,\n",
    "  options=None,\n",
    "  save_traces=True,)\n",
    "# model.save(model_name, save_format='tf')\n",
    "\n",
    "# ## save weight\n",
    "# model_weight = \"pplcntr_\" + dt.now().strftime(\"%Y%m%d-%H%M%S\") + \".h5\"\n",
    "# model.save_weights(model_weight, save_format='h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g_n2VfMsg1NT",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "e20fc8d4-968c-4272-9c62-57eeefcee1b3"
   },
   "source": [
    "model.evaluate(test_x, test_y)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oNuY-45SngR",
    "colab_type": "text"
   },
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VTfYjsyJTEtv",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# create some sample data\n",
    "X, Y = convert(x_test, y_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QPazH1zFTnE4",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "535d81f5-7835-47ce-98a0-6d78bfe13c78"
   },
   "source": [
    "# get prediction for one sample\n",
    "y_pred = model.predict(X)\n",
    "print(y_pred.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jrD03dgjcZMO",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "OBJperCLASS = 10 # get the top 10 results for each class\n",
    "# get the confidence scores (with class values) and delta for the boxes. For each class, the top 10 values are used\n",
    "def infer(Y):\n",
    "  # classes are actually the index into the default boxes\n",
    "  classes = np.zeros((OBJperCLASS,outputChannels-4),dtype=np.uint16)\n",
    "  conf = np.zeros((OBJperCLASS,outputChannels-4))\n",
    "  delta = np.zeros((OBJperCLASS,outputChannels-4,4))\n",
    "  class_predictions = softmax(Y[:,:outputChannels-4],axis=1)\n",
    "  for i in range(outputChannels-4):\n",
    "    classes[:,i] = Bottleneck.argpartition(class_predictions[:,i],BOXES-1-10,axis=-1)[-OBJperCLASS:]\n",
    "    conf[:,i] = class_predictions[classes[:,i],i]\n",
    "    delta[:,i] = Y[classes[:,i],outputChannels-4:]\n",
    "\n",
    "  return conf,classes, delta\n",
    "\n",
    "\n",
    "# generate bounding boxes from the inferred outputs\n",
    "def Bbox(confidence,box_idx,delta):\n",
    "  #delta contains delta(cx,cy,h,w)\n",
    "  bbox_centre = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
    "  bbox_hw = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
    "  for i in range(OBJperCLASS):\n",
    "    bbox_centre[i,:,0] = centres[box_idx[i]][:,0]+delta[i,:,0]\n",
    "    bbox_centre[i,:,1] = centres[box_idx[i]][:,1]+delta[i,:,1]\n",
    "    bbox_hw[i,:,0] = hw[box_idx[i]][:,0] + delta[i,:,2]\n",
    "    bbox_hw[i,:,1] = hw[box_idx[i]][:,1]+delta[i,:,3]\n",
    "\n",
    "  return bbox_centre,bbox_hw"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LY7SOlpafX51",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "outputId": "eef5e296-27f6-46f1-bfeb-422573f1d1e1"
   },
   "source": [
    "r = np.random.randint(TESTSIZE)\n",
    "\n",
    "# top 10 predictions for each class\n",
    "confidence, box_idx, delta = infer(y_pred[r])\n",
    "bbox_centre,bbox_hw = Bbox(confidence, box_idx, delta)\n",
    "\n",
    "im = np.array(Image.fromarray(X[r].astype(np.uint8)))\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow(im)\n",
    "\n",
    "for i in range(outputChannels-4):\n",
    "  # skipping backgrounds\n",
    "  if i == NUM_CLASSES:\n",
    "    continue\n",
    "  color = 'r'\n",
    "  # if a class is mentioned in the ground truth, color the boxes green\n",
    "  if i in Y[r,:,0]:\n",
    "    color = 'g'\n",
    "    print(i)\n",
    "  \n",
    "  # skip all the classes which have low confidence values\n",
    "  if (confidence[:,i] > 0.5).any() or i in Y[r,:,0]:\n",
    "    for k in range(OBJperCLASS):\n",
    "      print(\"{}: Confidence-{}\\t\\tCentre-{} Height,Width-{}\".format(i,confidence[k,i],bbox_centre[k,i],bbox_hw[k,i]))\n",
    "      \n",
    "      # draw bounding box only if confidence scores are high\n",
    "      if confidence[k,i] < 0.5:\n",
    "        continue\n",
    "      x = bbox_centre[k,i,0] - bbox_hw[k,i,0]/2\n",
    "      y = bbox_centre[k,i,1] - bbox_hw[k,i,1]/2\n",
    "      rect = patches.Rectangle((y,x),bbox_hw[k,i,1],bbox_hw[k,i,0],linewidth=1,edgecolor=color,facecolor='none')\n",
    "      ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}