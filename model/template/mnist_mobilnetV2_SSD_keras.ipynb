{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Mobilenetv2 + SSD.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbsreEfjnQtP",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Implementation of MobileNetv2+SSD<br>\n",
    "This is an implementation of the MobileNetv2 + SSD architecture for a relatively simpler task of determining bounding boxes for MNIST images embedded in a box. Each box contains only one digit(28x28 MNIST embedded into a 224x224 box) as of now, but the number of predictions per image can be expanded easily (the training outputs need to modified). Also, no data augmentation has been used till now (Colab kept crashing when I increased the dataset size beyond 1000, so the initial amount of data present was sufficient. The crashes might have been due to high traffic, but I haven't confirmed it).<p>\n",
    "In the earlier implementation, the ground truth data contained information about only one bounding box, which meant only one prediction per image ( reference https://colab.research.google.com/github/rs9899/mySSDimplementation/blob/master/MobileNetSSD_v2.ipynb#scrollTo=xWBzDsvkDqx5). For me, it also reduced the training signal and the model was overfitting. So I changed the outputs to a prediction for each default box (as it should be, from what I understood from the SSD paper). Although the initial implementation is good for the purposes for understanding the model.\n",
    "\n",
    "Comments mentioned throughout the code mention what needs to change if the model inputs or outputs are changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdL2AL8yAi00",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_hvSY3X-AeFP",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "1039d725-6ddc-4032-f23c-a33ccd056bc8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy.matlib\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "# import Bottleneck"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFmrlFJ8uKCe",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define Bottleneck Residual layer for MobileNet<br>\n",
    "Using the same parameters as mentioned in the paper"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xmwhyyS7CFyK",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class Bottleneck(tf.keras.Model):\n",
    "  def __init__(self, expansion, filters, stride, block_id, alpha=1, ):\n",
    "    super(Bottleneck, self).__init__(name=\"Bottleneck_\" + block_id)\n",
    "    self.expansion = expansion\n",
    "    self.alpha = alpha\n",
    "    self.output_channels = self.alpha * filters\n",
    "    self.stride = stride\n",
    "    self.out = None  # there was some problem with the eager execution\n",
    "\n",
    "    prefix = 'Bottleneck_{}_'.format(block_id)\n",
    "    self.prefix = prefix\n",
    "    # expansion\n",
    "    self.expand_BN = layers.BatchNormalization(name=prefix + 'expand_BN')\n",
    "    self.expand_ReLU = layers.ReLU(max_value=6, name=prefix + 'expand_ReLU')\n",
    "\n",
    "    #conv\n",
    "    self.Conv = layers.DepthwiseConv2D(kernel_size=3, padding='same', strides=self.stride, use_bias=False, name=prefix + 'conv')\n",
    "    self.Conv_BN = layers.BatchNormalization(name=prefix + 'conv_BN')\n",
    "    self.Conv_ReLU = layers.ReLU(max_value=6, name=prefix + 'conv_ReLU')\n",
    "\n",
    "    #project\n",
    "    self.project = layers.Conv2D(filters=self.output_channels, kernel_size=1, use_bias=False, name='contract')\n",
    "    self.project_BN = layers.BatchNormalization(name=prefix + 'contract_BN')\n",
    "\n",
    "    # dimensions need to be the same for residual connection\n",
    "    self.residual = layers.Add(name=prefix + 'residual')\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.d = input_shape[-1]\n",
    "    self.expand = layers.Conv2D(filters=self.expansion * self.d, kernel_size=1, use_bias=False, name=self.prefix + 'expand')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.expand(inputs)\n",
    "    x = self.expand_BN(x)\n",
    "    x = self.expand_ReLU(x)\n",
    "    self.out = x\n",
    "\n",
    "    x = self.Conv(x)\n",
    "    x = self.Conv_BN(x)\n",
    "    x = self.Conv_ReLU(x)\n",
    "\n",
    "    x = self.project(x)\n",
    "    x = self.project_BN(x)\n",
    "\n",
    "    if self.output_channels == self.d and self.stride == 1:\n",
    "      x = self.residual([inputs, x])\n",
    "\n",
    "    return x\n",
    "\n",
    "  def model(self):\n",
    "    x = tf.keras.Input(shape=(28, 28, 3))\n",
    "    return tf.keras.Model(inputs=[x], outputs=self.call(x))"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VYGagWE8T2Et",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class MobileNetv2(tf.keras.Model):\n",
    "  def __init__(self, k=11):\n",
    "    super(MobileNetv2, self).__init__()\n",
    "    self.k = k\n",
    "\n",
    "    self.pad = layers.ZeroPadding2D(padding=2, name='pad')\n",
    "    self.conv_inp = layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), padding='valid', use_bias=False, name='conv')\n",
    "    self.BN = layers.BatchNormalization(name='BN')\n",
    "    self.ReLU = layers.ReLU(max_value=6, name='ReLU')\n",
    "\n",
    "    self.B1_1 = Bottleneck(expansion=1, filters=16, stride=1, block_id='B1_1')\n",
    "\n",
    "    self.B2_1 = Bottleneck(expansion=6, filters=24, stride=2, block_id='B2_1')\n",
    "    self.B2_2 = Bottleneck(expansion=6, filters=24, stride=1, block_id='B2_2')\n",
    "\n",
    "    self.B3_1 = Bottleneck(expansion=6, filters=32, stride=2, block_id='B3_1')\n",
    "    self.B3_2 = Bottleneck(expansion=6, filters=32, stride=1, block_id='B3_2')\n",
    "    self.B3_3 = Bottleneck(expansion=6, filters=32, stride=1, block_id='B3_3')\n",
    "\n",
    "    self.B4_1 = Bottleneck(expansion=6, filters=64, stride=2, block_id='B4_1')\n",
    "    self.B4_2 = Bottleneck(expansion=6, filters=64, stride=1, block_id='B4_2')\n",
    "    self.B4_3 = Bottleneck(expansion=6, filters=64, stride=1, block_id='B4_3')\n",
    "    self.B4_4 = Bottleneck(expansion=6, filters=64, stride=1, block_id='B4_4')\n",
    "\n",
    "    self.B5_1 = Bottleneck(expansion=6, filters=96, stride=1, block_id='B5_1')\n",
    "    self.B5_2 = Bottleneck(expansion=6, filters=96, stride=1, block_id='B5_2')\n",
    "    self.B5_3 = Bottleneck(expansion=6, filters=96, stride=1, block_id='B5_3')\n",
    "\n",
    "    self.B6_1 = Bottleneck(expansion=6, filters=160, stride=2, block_id='B6_1')\n",
    "    self.B6_2 = Bottleneck(expansion=6, filters=160, stride=1, block_id='B6_2')\n",
    "    self.B6_3 = Bottleneck(expansion=6, filters=160, stride=1, block_id='B6_3')\n",
    "\n",
    "    self.B7_1 = Bottleneck(expansion=6, filters=320, stride=1, block_id='B7_1')\n",
    "\n",
    "    self.conv_out = layers.Conv2D(filters=1280, kernel_size=1, strides=(1, 1), use_bias=False, name='conv_out')\n",
    "    self.avgpool = layers.AveragePooling2D(pool_size=(7, 7), name='avg_pool')\n",
    "    self.conv_seg = layers.Conv2D(filters=self.k, kernel_size=1, strides=(1, 1), use_bias=False, name='conv_seg')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.conv_inp(inputs)\n",
    "    x = self.BN(x)\n",
    "    x = self.ReLU(x)\n",
    "\n",
    "    x = self.B1_1(x)\n",
    "\n",
    "    x = self.B2_1(x)\n",
    "    x = self.B2_2(x)\n",
    "\n",
    "    x = self.B3_1(x)\n",
    "    x = self.B3_2(x)\n",
    "    x = self.B3_3(x)\n",
    "\n",
    "    x = self.B4_1(x)\n",
    "    x = self.B4_2(x)\n",
    "    x = self.B4_3(x)\n",
    "    x = self.B4_4(x)\n",
    "\n",
    "    x = self.B5_1(x)\n",
    "    x = self.B5_2(x)\n",
    "    x = self.B5_3(x)\n",
    "\n",
    "    x = self.B6_1(x)\n",
    "    x = self.B6_2(x)\n",
    "    x = self.B6_3(x)\n",
    "\n",
    "    x = self.B7_1(x)\n",
    "\n",
    "    x = self.conv_out(x)\n",
    "    x = self.avgpool(x)\n",
    "    c4 = self.conv_seg(x)\n",
    "\n",
    "    return c4\n",
    "\n",
    "  def model(self):\n",
    "    x = tf.keras.Input(shape=(224, 224, 3))\n",
    "    return tf.keras.Model(inputs=x, outputs=self.call(x))"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "05i7363EYwmP",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "outputId": "d8434d3b-e85a-4481-b5bd-15751d6e9140",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "MobileNetv2().model().summary()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 18:27:54.438305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 18:27:54.442501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 18:27:54.442674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 18:27:54.443362: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-27 18:27:54.444035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 18:27:54.444190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 18:27:54.444323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 18:27:54.762983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 18:27:54.763174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 18:27:54.763322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 18:27:54.763452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 685 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv (Conv2D)               (None, 111, 111, 32)      864       \n",
      "                                                                 \n",
      " BN (BatchNormalization)     (None, 111, 111, 32)      128       \n",
      "                                                                 \n",
      " ReLU (ReLU)                 (None, 111, 111, 32)      0         \n",
      "                                                                 \n",
      " Bottleneck_B1_1 (Bottleneck  (None, 111, 111, 16)     2144      \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B2_1 (Bottleneck  (None, 56, 56, 24)       5568      \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B2_2 (Bottleneck  (None, 56, 56, 24)       9456      \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B3_1 (Bottleneck  (None, 28, 28, 32)       10640     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B3_2 (Bottleneck  (None, 28, 28, 32)       15680     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B3_3 (Bottleneck  (None, 28, 28, 32)       15680     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B4_1 (Bottleneck  (None, 14, 14, 64)       21952     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B4_2 (Bottleneck  (None, 14, 14, 64)       55936     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B4_3 (Bottleneck  (None, 14, 14, 64)       55936     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B4_4 (Bottleneck  (None, 14, 14, 64)       55936     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B5_1 (Bottleneck  (None, 14, 14, 96)       68352     \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B5_2 (Bottleneck  (None, 14, 14, 96)       120768    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B5_3 (Bottleneck  (None, 14, 14, 96)       120768    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B6_1 (Bottleneck  (None, 7, 7, 160)        157888    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B6_2 (Bottleneck  (None, 7, 7, 160)        324160    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B6_3 (Bottleneck  (None, 7, 7, 160)        324160    \n",
      " )                                                               \n",
      "                                                                 \n",
      " Bottleneck_B7_1 (Bottleneck  (None, 7, 7, 320)        478400    \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_out (Conv2D)           (None, 7, 7, 1280)        409600    \n",
      "                                                                 \n",
      " avg_pool (AveragePooling2D)  (None, 1, 1, 1280)       0         \n",
      "                                                                 \n",
      " conv_seg (Conv2D)           (None, 1, 1, 11)          14080     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,268,096\n",
      "Trainable params: 2,236,480\n",
      "Non-trainable params: 31,616\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II03ZRu17FnE",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SSD\n",
    "The default number of boxes per layer and resolution of each layer is different, since we are working with MNIST data and 224x224 image sizes.\n",
    "To change the number of boxes per layer and layerWidths, some constraints need to be kept in mind which are mentioned in the later sections"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aYD2gfR9O8L0",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class SSD(tf.keras.Model):\n",
    "  def __init__(self, numBoxes=[4,6,6,6,4,4], layerWidth=[28,14,7,4,2,1], k=10+1+4):\n",
    "    super(SSD,self).__init__()\n",
    "    self.classes = k\n",
    "    self.featureMaps = 6\n",
    "    self.MobileNet = MobileNetv2(k=k)\n",
    "    self.numBoxes = numBoxes\n",
    "    self.layerWidth = layerWidth\n",
    "    self.features = [None for _ in range(self.featureMaps)]\n",
    "    self.classifiers = [None for _ in range(self.featureMaps)]\n",
    "\n",
    "    self.conv1_1 = layers.Conv2D(256, 1, name='SSD_conv_1_1')\n",
    "    self.conv1_2 = layers.Conv2D(512, 3, strides=(2,2), padding='same', name='SSD_conv_1_2')\n",
    "\n",
    "    self.conv2_1 = layers.Conv2D(128, 1, name='SSD_conv_2_1')\n",
    "    self.conv2_2 = layers.Conv2D(256, 3, strides=(2,2), padding='same', name='SSD_conv_2_2')\n",
    "\n",
    "    self.conv3_1 = layers.Conv2D(128, 1, name='SSD_conv_3_1')\n",
    "    self.conv3_2 = layers.Conv2D(256, 3, strides=(1,1), name='SSD_conv_3_2')\n",
    "\n",
    "    self.conv4_1 = layers.Conv2D(128, 1, name='SSD_conv_4_1')\n",
    "    self.conv4_2 = layers.Conv2D(256, 2, strides=(1,1), name='SSD_conv_4_2') # changed the kernel size to 2 since the output of the previous layer has width 3\n",
    "    self.conv = []\n",
    "    self.reshape = []\n",
    "\n",
    "    for i in range(self.featureMaps):\n",
    "      self.conv.append(layers.Conv2D(self.numBoxes[i]*self.classes, 3, padding='same', name='Classification_'+str(i)))\n",
    "      self.reshape.append(layers.Reshape((self.layerWidth[i]*self.layerWidth[i]*self.numBoxes[i], self.classes),name='Reshape_classification_'+str(i)))\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.MobileNet.build(input_shape)\n",
    "\n",
    "  def call(self,inputs):\n",
    "    x = inputs\n",
    "    x = self.MobileNet(x)\n",
    "\n",
    "    # get the convolved images at different resolutions\n",
    "    self.features[0] = self.MobileNet.get_layer('Bottleneck_B4_1').out\n",
    "    self.features[1] = self.MobileNet.get_layer('Bottleneck_B5_3').out\n",
    "    self.features[2] = self.conv1_2(self.conv1_1(self.features[1]))\n",
    "    self.features[3] = self.conv2_2(self.conv2_1(self.features[2]))\n",
    "    self.features[4] = self.conv3_2(self.conv3_1(self.features[3]))\n",
    "    self.features[5] = self.conv4_2(self.conv4_1(self.features[4]))\n",
    "\n",
    "    for i in range(self.featureMaps):\n",
    "      # for each feature map, create predictions according to the number of boxes for that layer and the number of output channels\n",
    "      x = self.conv[i](self.features[i])\n",
    "      x = self.reshape[i](x)\n",
    "      self.classifiers[i] = x\n",
    "\n",
    "    # concatenate all the classifiers\n",
    "    x = layers.concatenate(self.classifiers, axis=-2, name='concatenate')\n",
    "    return x\n",
    "\n",
    "  def model(self):\n",
    "    x = tf.keras.Input(shape=(224,224,3))\n",
    "    return tf.keras.Model(inputs=x, outputs=self.call(x))"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "29A_FW-GxK4t",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "NUM_CLASSES = 10\n",
    "# the first 2 dimensions should be equal to width of the output from the bottleneck expand ReLU at the (4,1) and (5,3) respectively.\n",
    "# the dimensions after the second one are determined by the convolutions written inside the SSD (conv1_2, conv2_2, conv3_3, conv4_2)\n",
    "layerWidths = [28,14,7,4,2,1]\n",
    "numBoxes = [3,3,3,3,3,3]\n",
    "assert len(numBoxes) == len(layerWidths) # numBoxes for each layer and each layer has a specific width\n",
    "outputChannels = NUM_CLASSES + 1 + 4 # 10 classes + background + cx,cy,h,w\n",
    "assert outputChannels - NUM_CLASSES == 5"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FmOfPVIjCkuP",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "5fdfe35e-e5d8-4f1f-c112-da87335a2f74",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "model = SSD(numBoxes=numBoxes, layerWidth=layerWidths, k=outputChannels)\n",
    "model.model().summary()"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"SSD_conv_1_1\" (type Conv2D).\n\n<tf.Tensor 'mobile_netv2_2/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6:0' shape=(None, 14, 14, 576) dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\nPlease see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n\n<tf.Tensor 'mobile_netv2_2/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6:0' shape=(None, 14, 14, 576) dtype=float32> was defined here:\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 972, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2880, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2935, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3134, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3337, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3397, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_248627/3485494895.py\", line 2, in <cell line: 2>\n      model.model().summary()\n    File \"/tmp/ipykernel_248627/2245351542.py\", line 57, in model\n      return tf.keras.Model(inputs=x, outputs=self.call(x))\n    File \"/tmp/ipykernel_248627/2245351542.py\", line 35, in call\n      x = self.MobileNet(x)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 944, in __call__\n      return self._functional_construction_call(inputs, args, kwargs,\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 2315, in _functional_construction_call\n      outputs = self._keras_tensor_symbolic_call(\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 2186, in _keras_tensor_symbolic_call\n      return self._infer_output_signature(inputs, args, kwargs, input_masks)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 2232, in _infer_output_signature\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/tmp/ipykernel_248627/2890307493.py\", line 60, in call\n      x = self.B5_3(x)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/tmp/ipykernel_248627/333962661.py\", line 35, in call\n      x = self.expand_ReLU(x)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/layers/activation/relu.py\", line 96, in call\n      return backend.relu(inputs,\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 1082, in op_dispatch_handler\n      return dispatch_target(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/backend.py\", line 4989, in relu\n      x = tf.nn.relu6(x)\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 1082, in op_dispatch_handler\n      return dispatch_target(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\", line 3627, in relu6\n      return gen_nn_ops.relu6(features, name=name)\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 10616, in relu6\n      _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 797, in _apply_op_helper\n      op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 694, in _create_op_internal\n      return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3754, in _create_op_internal\n      ret = Operation(\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 2133, in __init__\n      self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n\nThe tensor <tf.Tensor 'mobile_netv2_2/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6:0' shape=(None, 14, 14, 576) dtype=float32> cannot be accessed from here, because it was defined in FuncGraph(name=mobile_netv2_2_scratch_graph, id=139964200281952), which is out of scope.\n\nCall arguments received by layer \"SSD_conv_1_1\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(None, 14, 14, 576), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m SSD(numBoxes\u001B[38;5;241m=\u001B[39mnumBoxes, layerWidth\u001B[38;5;241m=\u001B[39mlayerWidths, k\u001B[38;5;241m=\u001B[39moutputChannels)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msummary()\n",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36mSSD.model\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmodel\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     56\u001B[0m   x \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mInput(shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m224\u001B[39m,\u001B[38;5;241m224\u001B[39m,\u001B[38;5;241m3\u001B[39m))\n\u001B[0;32m---> 57\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mModel(inputs\u001B[38;5;241m=\u001B[39mx, outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36mSSD.call\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mMobileNet\u001B[38;5;241m.\u001B[39mget_layer(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBottleneck_B4_1\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mout\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mMobileNet\u001B[38;5;241m.\u001B[39mget_layer(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBottleneck_B5_3\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mout\n\u001B[0;32m---> 40\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv1_2(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1_1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2_2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2_1(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures[\u001B[38;5;241m2\u001B[39m]))\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures[\u001B[38;5;241m4\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv3_2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv3_1(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures[\u001B[38;5;241m3\u001B[39m]))\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[1;32m     55\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mTypeError\u001B[0m: Exception encountered when calling layer \"SSD_conv_1_1\" (type Conv2D).\n\n<tf.Tensor 'mobile_netv2_2/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6:0' shape=(None, 14, 14, 576) dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\nPlease see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n\n<tf.Tensor 'mobile_netv2_2/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6:0' shape=(None, 14, 14, 576) dtype=float32> was defined here:\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 972, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2880, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2935, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3134, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3337, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3397, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_248627/3485494895.py\", line 2, in <cell line: 2>\n      model.model().summary()\n    File \"/tmp/ipykernel_248627/2245351542.py\", line 57, in model\n      return tf.keras.Model(inputs=x, outputs=self.call(x))\n    File \"/tmp/ipykernel_248627/2245351542.py\", line 35, in call\n      x = self.MobileNet(x)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 944, in __call__\n      return self._functional_construction_call(inputs, args, kwargs,\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 2315, in _functional_construction_call\n      outputs = self._keras_tensor_symbolic_call(\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 2186, in _keras_tensor_symbolic_call\n      return self._infer_output_signature(inputs, args, kwargs, input_masks)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 2232, in _infer_output_signature\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/tmp/ipykernel_248627/2890307493.py\", line 60, in call\n      x = self.B5_3(x)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/tmp/ipykernel_248627/333962661.py\", line 35, in call\n      x = self.expand_ReLU(x)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/layers/activation/relu.py\", line 96, in call\n      return backend.relu(inputs,\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 1082, in op_dispatch_handler\n      return dispatch_target(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/keras/backend.py\", line 4989, in relu\n      x = tf.nn.relu6(x)\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 1082, in op_dispatch_handler\n      return dispatch_target(*args, **kwargs)\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\", line 3627, in relu6\n      return gen_nn_ops.relu6(features, name=name)\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 10616, in relu6\n      _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 797, in _apply_op_helper\n      op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 694, in _create_op_internal\n      return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3754, in _create_op_internal\n      ret = Operation(\n    File \"/home/z/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 2133, in __init__\n      self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n\nThe tensor <tf.Tensor 'mobile_netv2_2/Bottleneck_B5_3/Bottleneck_B5_3_expand_ReLU/Relu6:0' shape=(None, 14, 14, 576) dtype=float32> cannot be accessed from here, because it was defined in FuncGraph(name=mobile_netv2_2_scratch_graph, id=139964200281952), which is out of scope.\n\nCall arguments received by layer \"SSD_conv_1_1\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(None, 14, 14, 576), dtype=float32)"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RA7NxfQ7_G6",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating boxes and IoU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yRYi7Ez7UzpH",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# I have used less varying custom scales and aspect ratios here, since the dataset is already uniform\n",
    "#IMPORTANT: before changing the scales and aspect ratios, read the comment below\n",
    "\n",
    "# number of scales is equal to the number of different resolutions ie num of layer widths\n",
    "# for a given resolution, we have different aspect ratios\n",
    "# num(scales) = num(layerWidth) = num(numBoxes) and num(asp_ratios) = numBoxes[i]\n",
    "MinScale = .1 # Min and Max scale given as percentage\n",
    "MaxScale = 1.5\n",
    "scales = [ MinScale + x/len(layerWidths) * (MaxScale-MinScale) for x in range(len(layerWidths)) ]\n",
    "scales = scales[::-1] # reversing the order because the layerWidths go from high to low (lower to higher resoltuion)\n",
    "\n",
    "asp = [0.5,1.0,1.5]\n",
    "asp1 = [x**0.5 for x in asp]\n",
    "asp2 = [1/x for x in asp1]\n",
    "IMG_SIZE = 224\n",
    "# should be equal to the 1st dimension in the output layer of the SSD model\n",
    "BOXES = sum([a*a*b for a,b in zip(layerWidths,numBoxes)])\n",
    "centres = np.zeros((BOXES,2))\n",
    "hw = np.zeros((BOXES,2))\n",
    "boxes = np.zeros((BOXES,4))\n",
    "print(BOXES)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9A1xGMrXVX18",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# calculating the default box centres and height, width\n",
    "idx = 0\n",
    "\n",
    "for gridSize, numBox, scale in zip(layerWidths,numBoxes,scales):\n",
    "  step_size = IMG_SIZE*1.0/gridSize\n",
    "  for i in range(gridSize):\n",
    "    for j in range(gridSize):\n",
    "      pos = idx + (i*gridSize+j) * numBox\n",
    "      # centre is the same for all aspect ratios(=numBox)\n",
    "      centres[ pos : pos + numBox , :] = i*step_size + step_size/2, j*step_size + step_size/2\n",
    "      # height and width vary according to the scale and aspect ratio\n",
    "      # zip asepct ratios and then scale them by the scaling factor\n",
    "      hw[ pos : pos + numBox , :] = np.multiply(gridSize*scale, np.squeeze(np.dstack([asp1,asp2]),axis=0))[:numBox,:]\n",
    "\n",
    "  idx += gridSize*gridSize*numBox\n",
    "\n",
    "# (x,y) co-ordinates of top left and bottom right\n",
    "# This actually is not used anywhere. centres[] and hw[] are a good enough substitute\n",
    "boxes[:,0] = centres[:,0] - hw[:,0]/2\n",
    "boxes[:,1] = centres[:,1] - hw[:,1]/2\n",
    "boxes[:,2] = centres[:,0] + hw[:,0]/2\n",
    "boxes[:,3] = centres[:,1] + hw[:,1]/2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TJSIPHPMh3N2",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# calculate IoU for a set of search boxes and default boxes\n",
    "def IoU(box1, box2):\n",
    "  box1 = box1.astype(np.float64)\n",
    "  box2 = box2.astype(np.float64)\n",
    "  # find the left and right co-ordinates of the edges. Min should be less than Max for non zero overlap\n",
    "  xmin = np.maximum(box1[:,0],box2[:,0])\n",
    "  xmax = np.minimum(box1[:,2],box2[:,2])\n",
    "  ymin = np.maximum(box1[:,1],box2[:,1])\n",
    "  ymax = np.minimum(box1[:,3],box2[:,3])\n",
    "\n",
    "  intersection = np.abs(np.maximum(xmax-xmin,0) * np.maximum(ymax-ymin,0))\n",
    "  boxArea1 = np.abs((box1[:,2] - box1[:,0]) * (box1[:,3] - box1[:,1]))\n",
    "  boxArea2 = np.abs((box2[:,2] - box2[:,0]) * (box2[:,3] - box2[:,1]))\n",
    "  unionArea = boxArea1 + boxArea2 - intersection\n",
    "  assert (unionArea > 0).all()\n",
    "  iou = intersection / unionArea\n",
    "\n",
    "  return iou\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "# give the index of the box correpsonding to the IoUs > threshold (=0.5)\n",
    "def bestIoU(searchBox):\n",
    "  return np.argwhere(IoU(numpy.matlib.repmat(searchBox,BOXES,1), boxes) > THRESHOLD)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm2k4c5Ik_BX",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TRAINSIZE = 600\n",
    "TESTSIZE = 100\n",
    "\n",
    "x_train = x_train[:TRAINSIZE, : , :]\n",
    "y_train = y_train[:TRAINSIZE]\n",
    "x_test = x_test[:TESTSIZE, : , :]\n",
    "y_test = y_test[:TESTSIZE]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert data for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PEgx1Sdcq7yJ",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# take mnist x and y pairs and convert to input, output pairs for the MobileNetv2+SSD model\n",
    "def convert(x,y):\n",
    "  MNIST_SIZE = x.shape[-1]\n",
    "  # create a 2D array of top left corners for the mnist image to be placed\n",
    "  corner = np.random.randint(IMG_SIZE - MNIST_SIZE, size=(x.shape[0],2))\n",
    "\n",
    "  # create a blank canvas for the input with the required dimension\n",
    "  input = np.zeros((x.shape[0], IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "  # replacing a part by RGB version of MNIST\n",
    "  for i in range(x.shape[0]):\n",
    "    lx = int(corner[i,0])\n",
    "    ly = int(corner[i,1])\n",
    "    input[i,lx:lx + MNIST_SIZE, ly:ly+MNIST_SIZE,:] = np.repeat(np.expand_dims(np.array(x[i,:,:]),axis=-1),3,axis=-1)\n",
    "\n",
    "  # for each default box, there are 5 values: class number and delta cx,cy,h,w\n",
    "  output = np.zeros((y.shape[0],BOXES,1+4))\n",
    "  output[:,:,0] = NUM_CLASSES # defaulting class labels for all boxes to background initially\n",
    "  for i in range(x.shape[0]):\n",
    "    bbox = np.zeros(4)\n",
    "    bbox[:2] = corner[i]\n",
    "    bbox[2:] = corner[i] + (MNIST_SIZE,MNIST_SIZE)\n",
    "    # for all default boxes which have IoU > threshold, set the delta values and class number\n",
    "    box_idx = bestIoU(bbox).astype(np.uint16)\n",
    "    output[i,box_idx,0] = y[i]\n",
    "    output[i,box_idx,1] = (bbox[0] + bbox[2])/2.0 - centres[box_idx,0]\n",
    "    output[i,box_idx,2] = (bbox[1] + bbox[3])/2.0 - centres[box_idx,1]\n",
    "    output[i,box_idx,3] = MNIST_SIZE - hw[box_idx,0]\n",
    "    output[i,box_idx,4] = MNIST_SIZE - hw[box_idx,1]\n",
    "\n",
    "  return input, output\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Sk_z17wV3Bj5",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "test_x, test_y = convert(x_test,y_test)\n",
    "train_x, train_y = convert(x_train,y_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NAwnJnu4qE0P",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "outputId": "27116f08-73e6-42e7-c8d8-daac27654afc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# checking if the inputs prepared are correct or not\n",
    "r = np.random.randint(0,train_x.shape[0])\n",
    "img = train_x[r,:,:,:].copy()\n",
    "img_y = train_y[r]\n",
    "\n",
    "im = np.array(Image.fromarray(img.astype(np.uint8)))\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow(im)\n",
    "\n",
    "# find all boxes where class label is not background\n",
    "idx = np.argwhere(img_y[:,0] != NUM_CLASSES)[:,0]\n",
    "print('Number of boxes with IoU > 0.5:',idx.shape[0])\n",
    "print('Green box: ground truth. Red box: default boxes with IoU > threshold')\n",
    "\n",
    "#calculating the ground truth bounding boxes\n",
    "gt = np.zeros(4,dtype=np.uint16)\n",
    "gt[:2] = (img_y[idx[0],1:3] + centres[idx[0],:2])\n",
    "gt[2:] = (img_y[idx[0],3:] + hw[idx[0],:])\n",
    "\n",
    "# for some reason, x and y are inverted\n",
    "rect = patches.Rectangle((gt[1]-gt[3]/2,gt[0]-gt[2]/2),gt[3],gt[2],linewidth=5,edgecolor='g',facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "\n",
    "# showing all the boxes with IoU > 0.5\n",
    "for i in idx:\n",
    "  rect = patches.Rectangle((centres[i][1]-hw[i,1]/2, centres[i][0]-hw[i,0]/2), hw[i,1], hw[i,0], linewidth=1, edgecolor='r', facecolor='none')\n",
    "  ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1nI7mXjS8zA9",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "88b50f5a-8931-4f30-eb69-f211e964d690",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "\n",
    "print(train_dataset.element_spec)\n",
    "print(test_dataset.element_spec)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oyX8dnwQ8_1k",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "BATCH_SIZE = 10\n",
    "SHUFFLE_BUFFER_SIZE = 60\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LOSS FUNCTION\n",
    "Hard negative mining hasn't been done here\n",
    "Initial idea was to assign weights to background classes, but there is some problem in that approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# label is not required here in the standard implementation\n",
    "# calculate the smooth L1 loss\n",
    "def smoothL1(x,y,label):\n",
    "  diff = K.abs(x-y) #* K.switch(label == 10, label*1.0/BOXES, label)\n",
    "  result = K.switch(diff < 1, 0.5 * diff**2, diff - 0.5)\n",
    "\n",
    "  return K.mean(result)\n",
    "\n",
    "\n",
    "def confidenceLoss(y,label):\n",
    "  unweighted_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(label, y)\n",
    "  # class_weights = tf.constant([[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0/BOXES]]*BOXES])\n",
    "  # weights = tf.reduce_sum(class_weights * y, axis = -1)\n",
    "  # weighted_loss = unweighted_loss * weights\n",
    "\n",
    "  return K.mean(unweighted_loss)\n",
    "\n",
    "\n",
    "def Loss(gt,y):\n",
    "  # shape of y is n * BOXES * output_channels\n",
    "  # shape of gt is n * BOXES * 5 \n",
    "  loss = 0\n",
    "  # localisation loss\n",
    "  loss += smoothL1(y[:,:,-4:],gt[:,:,-4:],gt[:,:,0:1])\n",
    "  # confidence loss\n",
    "  loss += confidenceLoss(y[:,:,:-4],tf.cast(gt[:,:,0],tf.int32))\n",
    "\n",
    "  return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TRAIN MODEL"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A8_O3V8DB_Gk",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001), loss=Loss)\n",
    "history = model.fit(train_dataset, epochs=2, validation_data=test_dataset)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## save model\n",
    "model_name = \"pplcntr_\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model.save(filepath=model_name, overwrite=True, include_optimizer=True, save_format='tf', signatures=None, options=None, save_traces=True, )\n",
    "# model.save(model_name, save_format='tf')\n",
    "\n",
    "# ## save weight\n",
    "# model_weight = \"pplcntr_\" + dt.now().strftime(\"%Y%m%d-%H%M%S\") + \".h5\"\n",
    "# model.save_weights(model_weight, save_format='h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g_n2VfMsg1NT",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "e20fc8d4-968c-4272-9c62-57eeefcee1b3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "model.evaluate(test_x, test_y)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oNuY-45SngR",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VTfYjsyJTEtv",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# create some sample data\n",
    "X, Y = convert(x_test, y_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QPazH1zFTnE4",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "535d81f5-7835-47ce-98a0-6d78bfe13c78",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# get prediction for one sample\n",
    "y_pred = model.predict(X)\n",
    "print(y_pred.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jrD03dgjcZMO",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "OBJperCLASS = 10 # get the top 10 results for each class\n",
    "# get the confidence scores (with class values) and delta for the boxes. For each class, the top 10 values are used\n",
    "def infer(Y):\n",
    "  # classes are actually the index into the default boxes\n",
    "  classes = np.zeros((OBJperCLASS,outputChannels-4),dtype=np.uint16)\n",
    "  conf = np.zeros((OBJperCLASS,outputChannels-4))\n",
    "  delta = np.zeros((OBJperCLASS,outputChannels-4,4))\n",
    "  class_predictions = softmax(Y[:,:outputChannels-4],axis=1)\n",
    "  for i in range(outputChannels-4):\n",
    "    classes[:,i] = Bottleneck.argpartition(class_predictions[:,i],BOXES-1-10,axis=-1)[-OBJperCLASS:]\n",
    "    conf[:,i] = class_predictions[classes[:,i],i]\n",
    "    delta[:,i] = Y[classes[:,i],outputChannels-4:]\n",
    "\n",
    "  return conf,classes, delta\n",
    "\n",
    "\n",
    "# generate bounding boxes from the inferred outputs\n",
    "def Bbox(confidence,box_idx,delta):\n",
    "  #delta contains delta(cx,cy,h,w)\n",
    "  bbox_centre = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
    "  bbox_hw = np.zeros((OBJperCLASS,outputChannels-4,2))\n",
    "  for i in range(OBJperCLASS):\n",
    "    bbox_centre[i,:,0] = centres[box_idx[i]][:,0]+delta[i,:,0]\n",
    "    bbox_centre[i,:,1] = centres[box_idx[i]][:,1]+delta[i,:,1]\n",
    "    bbox_hw[i,:,0] = hw[box_idx[i]][:,0] + delta[i,:,2]\n",
    "    bbox_hw[i,:,1] = hw[box_idx[i]][:,1]+delta[i,:,3]\n",
    "\n",
    "  return bbox_centre,bbox_hw"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LY7SOlpafX51",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "outputId": "eef5e296-27f6-46f1-bfeb-422573f1d1e1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "r = np.random.randint(TESTSIZE)\n",
    "\n",
    "# top 10 predictions for each class\n",
    "confidence, box_idx, delta = infer(y_pred[r])\n",
    "bbox_centre,bbox_hw = Bbox(confidence, box_idx, delta)\n",
    "\n",
    "im = np.array(Image.fromarray(X[r].astype(np.uint8)))\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow(im)\n",
    "\n",
    "for i in range(outputChannels-4):\n",
    "  # skipping backgrounds\n",
    "  if i == NUM_CLASSES:\n",
    "    continue\n",
    "  color = 'r'\n",
    "  # if a class is mentioned in the ground truth, color the boxes green\n",
    "  if i in Y[r,:,0]:\n",
    "    color = 'g'\n",
    "    print(i)\n",
    "  \n",
    "  # skip all the classes which have low confidence values\n",
    "  if (confidence[:,i] > 0.5).any() or i in Y[r,:,0]:\n",
    "    for k in range(OBJperCLASS):\n",
    "      print(\"{}: Confidence-{}\\t\\tCentre-{} Height,Width-{}\".format(i,confidence[k,i],bbox_centre[k,i],bbox_hw[k,i]))\n",
    "      \n",
    "      # draw bounding box only if confidence scores are high\n",
    "      if confidence[k,i] < 0.5:\n",
    "        continue\n",
    "      x = bbox_centre[k,i,0] - bbox_hw[k,i,0]/2\n",
    "      y = bbox_centre[k,i,1] - bbox_hw[k,i,1]/2\n",
    "      rect = patches.Rectangle((y,x),bbox_hw[k,i,1],bbox_hw[k,i,0],linewidth=1,edgecolor=color,facecolor='none')\n",
    "      ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}