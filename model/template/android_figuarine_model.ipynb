{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l4QQTXHHATDS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 00:41:46.624000: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/z/GWT/gap_sdk/install/workstation/lib\n",
      "2021-12-30 00:41:46.624024: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tflite_model_maker.config import ExportFormat, QuantizationConfig\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "\n",
    "from tflite_support import metadata\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WiAahdsQAdT7"
   },
   "outputs": [],
   "source": [
    "train_data = object_detector.DataLoader.from_pascal_voc(\n",
    "    'in/pplcntr/train',\n",
    "    'in/pplcntr/train',\n",
    "    ['people']\n",
    ")\n",
    "\n",
    "val_data = object_detector.DataLoader.from_pascal_voc(\n",
    "    'in/pplcntr/validate',\n",
    "    'in/pplcntr/validate',\n",
    "    ['people']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GZOojrDHAY1J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 00:41:58.790104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-30 00:41:58.790280: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/z/GWT/gap_sdk/install/workstation/lib\n",
      "2021-12-30 00:41:58.790329: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/z/GWT/gap_sdk/install/workstation/lib\n",
      "2021-12-30 00:41:58.790372: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/z/GWT/gap_sdk/install/workstation/lib\n",
      "2021-12-30 00:41:58.790414: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/z/GWT/gap_sdk/install/workstation/lib\n",
      "2021-12-30 00:41:58.790456: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/z/GWT/gap_sdk/install/workstation/lib\n",
      "2021-12-30 00:41:58.790498: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/z/GWT/gap_sdk/install/workstation/lib\n",
      "2021-12-30 00:41:58.790539: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/z/GWT/gap_sdk/install/workstation/lib\n",
      "2021-12-30 00:41:58.790580: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/z/GWT/gap_sdk/install/workstation/lib\n",
      "2021-12-30 00:41:58.790587: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-12-30 00:41:58.791033: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "spec = model_spec.get('efficientdet_lite0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MClfpsJAfda",
    "outputId": "cbfc9772-a3c7-4f60-a7af-a6032de27d2f",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 00:42:08.971316: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.7938 - cls_loss: 0.5706 - box_loss: 0.0045 - reg_l2_loss: 0.0655 - loss: 0.8593 - learning_rate: 0.0065 - gradient_norm: 3.2183"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 01:12:27.097238: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 1821s 409ms/step - det_loss: 0.7938 - cls_loss: 0.5706 - box_loss: 0.0045 - reg_l2_loss: 0.0655 - loss: 0.8593 - learning_rate: 0.0065 - gradient_norm: 3.2181 - val_det_loss: 0.8304 - val_cls_loss: 0.5771 - val_box_loss: 0.0051 - val_reg_l2_loss: 0.0667 - val_loss: 0.8971\n",
      "Epoch 2/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.6633 - cls_loss: 0.4895 - box_loss: 0.0035 - reg_l2_loss: 0.0679 - loss: 0.7311 - learning_rate: 0.0049 - gradient_norm: 3.5380"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 01:42:03.965631: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 1775s 402ms/step - det_loss: 0.6632 - cls_loss: 0.4895 - box_loss: 0.0035 - reg_l2_loss: 0.0679 - loss: 0.7311 - learning_rate: 0.0049 - gradient_norm: 3.5395 - val_det_loss: 0.7485 - val_cls_loss: 0.4706 - val_box_loss: 0.0056 - val_reg_l2_loss: 0.0690 - val_loss: 0.8175\n",
      "Epoch 3/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.6174 - cls_loss: 0.4585 - box_loss: 0.0032 - reg_l2_loss: 0.0701 - loss: 0.6876 - learning_rate: 0.0048 - gradient_norm: 3.4236"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 02:10:40.726280: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 1717s 389ms/step - det_loss: 0.6173 - cls_loss: 0.4584 - box_loss: 0.0032 - reg_l2_loss: 0.0701 - loss: 0.6874 - learning_rate: 0.0048 - gradient_norm: 3.4241 - val_det_loss: 0.7096 - val_cls_loss: 0.4459 - val_box_loss: 0.0053 - val_reg_l2_loss: 0.0712 - val_loss: 0.7808\n",
      "Epoch 4/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.6027 - cls_loss: 0.4515 - box_loss: 0.0030 - reg_l2_loss: 0.0722 - loss: 0.6749 - learning_rate: 0.0046 - gradient_norm: 3.2298"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 02:39:45.110336: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 1744s 395ms/step - det_loss: 0.6027 - cls_loss: 0.4515 - box_loss: 0.0030 - reg_l2_loss: 0.0722 - loss: 0.6749 - learning_rate: 0.0046 - gradient_norm: 3.2295 - val_det_loss: 0.6879 - val_cls_loss: 0.4712 - val_box_loss: 0.0043 - val_reg_l2_loss: 0.0731 - val_loss: 0.7609\n",
      "Epoch 5/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.5934 - cls_loss: 0.4483 - box_loss: 0.0029 - reg_l2_loss: 0.0738 - loss: 0.6673 - learning_rate: 0.0043 - gradient_norm: 3.1454"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 03:08:12.839930: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 1711s 388ms/step - det_loss: 0.5935 - cls_loss: 0.4484 - box_loss: 0.0029 - reg_l2_loss: 0.0738 - loss: 0.6673 - learning_rate: 0.0043 - gradient_norm: 3.1464 - val_det_loss: 0.7757 - val_cls_loss: 0.5221 - val_box_loss: 0.0051 - val_reg_l2_loss: 0.0745 - val_loss: 0.8502\n",
      "Epoch 6/20\n",
      "4411/4411 [==============================] - 30584s 7s/step - det_loss: 0.5576 - cls_loss: 0.4195 - box_loss: 0.0028 - reg_l2_loss: 0.0751 - loss: 0.6327 - learning_rate: 0.0040 - gradient_norm: 2.9958 - val_det_loss: 0.5056 - val_cls_loss: 0.3059 - val_box_loss: 0.0040 - val_reg_l2_loss: 0.0756 - val_loss: 0.5812\n",
      "Epoch 7/20\n",
      "4411/4411 [==============================] - 1939s 439ms/step - det_loss: 0.5294 - cls_loss: 0.3949 - box_loss: 0.0027 - reg_l2_loss: 0.0760 - loss: 0.6054 - learning_rate: 0.0037 - gradient_norm: 2.9374 - val_det_loss: 0.5357 - val_cls_loss: 0.3335 - val_box_loss: 0.0040 - val_reg_l2_loss: 0.0764 - val_loss: 0.6121\n",
      "Epoch 8/20\n",
      "4411/4411 [==============================] - 1698s 385ms/step - det_loss: 0.5222 - cls_loss: 0.3906 - box_loss: 0.0026 - reg_l2_loss: 0.0767 - loss: 0.5989 - learning_rate: 0.0033 - gradient_norm: 3.0465 - val_det_loss: 0.5170 - val_cls_loss: 0.2938 - val_box_loss: 0.0045 - val_reg_l2_loss: 0.0770 - val_loss: 0.5940\n",
      "Epoch 9/20\n",
      "4411/4411 [==============================] - 1725s 391ms/step - det_loss: 0.4883 - cls_loss: 0.3625 - box_loss: 0.0025 - reg_l2_loss: 0.0772 - loss: 0.5655 - learning_rate: 0.0029 - gradient_norm: 2.8974 - val_det_loss: 0.5321 - val_cls_loss: 0.3060 - val_box_loss: 0.0045 - val_reg_l2_loss: 0.0774 - val_loss: 0.6095\n",
      "Epoch 10/20\n",
      "4411/4411 [==============================] - 1869s 424ms/step - det_loss: 0.4859 - cls_loss: 0.3676 - box_loss: 0.0024 - reg_l2_loss: 0.0775 - loss: 0.5634 - learning_rate: 0.0025 - gradient_norm: 2.8858 - val_det_loss: 0.5262 - val_cls_loss: 0.3356 - val_box_loss: 0.0038 - val_reg_l2_loss: 0.0776 - val_loss: 0.6037\n",
      "Epoch 11/20\n",
      "4411/4411 [==============================] - 1828s 414ms/step - det_loss: 0.4987 - cls_loss: 0.3824 - box_loss: 0.0023 - reg_l2_loss: 0.0776 - loss: 0.5763 - learning_rate: 0.0021 - gradient_norm: 2.8884 - val_det_loss: 0.5119 - val_cls_loss: 0.3195 - val_box_loss: 0.0038 - val_reg_l2_loss: 0.0776 - val_loss: 0.5895\n",
      "Epoch 12/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.4758 - cls_loss: 0.3611 - box_loss: 0.0023 - reg_l2_loss: 0.0776 - loss: 0.5534 - learning_rate: 0.0017 - gradient_norm: 2.9558"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 14:39:39.936704: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 1840s 417ms/step - det_loss: 0.4757 - cls_loss: 0.3610 - box_loss: 0.0023 - reg_l2_loss: 0.0776 - loss: 0.5533 - learning_rate: 0.0017 - gradient_norm: 2.9552 - val_det_loss: 0.5202 - val_cls_loss: 0.3228 - val_box_loss: 0.0039 - val_reg_l2_loss: 0.0776 - val_loss: 0.5978\n",
      "Epoch 13/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.4977 - cls_loss: 0.3855 - box_loss: 0.0022 - reg_l2_loss: 0.0775 - loss: 0.5753 - learning_rate: 0.0013 - gradient_norm: 2.9905"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 15:09:47.839055: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 1808s 410ms/step - det_loss: 0.4977 - cls_loss: 0.3855 - box_loss: 0.0022 - reg_l2_loss: 0.0775 - loss: 0.5753 - learning_rate: 0.0013 - gradient_norm: 2.9903 - val_det_loss: 0.5251 - val_cls_loss: 0.3358 - val_box_loss: 0.0038 - val_reg_l2_loss: 0.0775 - val_loss: 0.6026\n",
      "Epoch 14/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.4653 - cls_loss: 0.3564 - box_loss: 0.0022 - reg_l2_loss: 0.0774 - loss: 0.5427 - learning_rate: 9.6625e-04 - gradient_norm: 3.0298"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 15:40:47.792270: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 1860s 422ms/step - det_loss: 0.4653 - cls_loss: 0.3563 - box_loss: 0.0022 - reg_l2_loss: 0.0774 - loss: 0.5427 - learning_rate: 9.6622e-04 - gradient_norm: 3.0296 - val_det_loss: 0.4656 - val_cls_loss: 0.2685 - val_box_loss: 0.0039 - val_reg_l2_loss: 0.0774 - val_loss: 0.5430\n",
      "Epoch 15/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.4920 - cls_loss: 0.3829 - box_loss: 0.0022 - reg_l2_loss: 0.0773 - loss: 0.5693 - learning_rate: 6.6282e-04 - gradient_norm: 3.1355"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 16:13:42.074868: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 1976s 448ms/step - det_loss: 0.4919 - cls_loss: 0.3829 - box_loss: 0.0022 - reg_l2_loss: 0.0773 - loss: 0.5692 - learning_rate: 6.6279e-04 - gradient_norm: 3.1351 - val_det_loss: 0.4500 - val_cls_loss: 0.2547 - val_box_loss: 0.0039 - val_reg_l2_loss: 0.0773 - val_loss: 0.5273\n",
      "Epoch 16/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.4606 - cls_loss: 0.3554 - box_loss: 0.0021 - reg_l2_loss: 0.0772 - loss: 0.5378 - learning_rate: 4.0949e-04 - gradient_norm: 3.2507"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 16:47:15.550329: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 2012s 456ms/step - det_loss: 0.4606 - cls_loss: 0.3553 - box_loss: 0.0021 - reg_l2_loss: 0.0772 - loss: 0.5378 - learning_rate: 4.0947e-04 - gradient_norm: 3.2509 - val_det_loss: 0.4320 - val_cls_loss: 0.2582 - val_box_loss: 0.0035 - val_reg_l2_loss: 0.0772 - val_loss: 0.5092\n",
      "Epoch 17/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.4572 - cls_loss: 0.3502 - box_loss: 0.0021 - reg_l2_loss: 0.0772 - loss: 0.5344 - learning_rate: 2.1319e-04 - gradient_norm: 3.1787"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 17:18:39.874804: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 1884s 427ms/step - det_loss: 0.4572 - cls_loss: 0.3502 - box_loss: 0.0021 - reg_l2_loss: 0.0772 - loss: 0.5344 - learning_rate: 2.1317e-04 - gradient_norm: 3.1791 - val_det_loss: 0.4151 - val_cls_loss: 0.2398 - val_box_loss: 0.0035 - val_reg_l2_loss: 0.0772 - val_loss: 0.4923\n",
      "Epoch 18/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.4714 - cls_loss: 0.3705 - box_loss: 0.0020 - reg_l2_loss: 0.0771 - loss: 0.5485 - learning_rate: 7.9271e-05 - gradient_norm: 3.0569"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 17:50:02.669561: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 1883s 427ms/step - det_loss: 0.4714 - cls_loss: 0.3705 - box_loss: 0.0020 - reg_l2_loss: 0.0771 - loss: 0.5485 - learning_rate: 7.9260e-05 - gradient_norm: 3.0573 - val_det_loss: 0.4165 - val_cls_loss: 0.2396 - val_box_loss: 0.0035 - val_reg_l2_loss: 0.0771 - val_loss: 0.4937\n",
      "Epoch 19/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.4486 - cls_loss: 0.3450 - box_loss: 0.0021 - reg_l2_loss: 0.0771 - loss: 0.5258 - learning_rate: 1.1380e-05 - gradient_norm: 3.1287"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 18:20:49.613616: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 1847s 419ms/step - det_loss: 0.4485 - cls_loss: 0.3449 - box_loss: 0.0021 - reg_l2_loss: 0.0771 - loss: 0.5257 - learning_rate: 1.1377e-05 - gradient_norm: 3.1280 - val_det_loss: 0.4166 - val_cls_loss: 0.2403 - val_box_loss: 0.0035 - val_reg_l2_loss: 0.0771 - val_loss: 0.4938\n",
      "Epoch 20/20\n",
      "4411/4411 [==============================] - ETA: 0s - det_loss: 0.4711 - cls_loss: 0.3685 - box_loss: 0.0021 - reg_l2_loss: 0.0771 - loss: 0.5483 - learning_rate: 1.1372e-05 - gradient_norm: 3.1794"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 18:49:32.643278: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411/4411 [==============================] - 1724s 391ms/step - det_loss: 0.4728 - cls_loss: 0.3701 - box_loss: 0.0021 - reg_l2_loss: 0.0771 - loss: 0.5499 - learning_rate: 1.1377e-05 - gradient_norm: 3.1810 - val_det_loss: 0.4182 - val_cls_loss: 0.2407 - val_box_loss: 0.0036 - val_reg_l2_loss: 0.0771 - val_loss: 0.4954\n"
     ]
    }
   ],
   "source": [
    "model = object_detector.create(train_data, model_spec=spec, batch_size=4, train_whole_model=True, epochs=20, validation_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUqEpcYwAg8L",
    "outputId": "b359dc94-4906-4b5b-e022-77030b7a122f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'AP': 0.46231443,\n 'AP50': 0.9160241,\n 'AP75': 0.39535406,\n 'APs': 0.46239898,\n 'APm': -1.0,\n 'APl': -1.0,\n 'ARmax1': 0.16,\n 'ARmax10': 0.558,\n 'ARmax100': 0.601,\n 'ARs': 0.601,\n 'ARm': -1.0,\n 'ARl': -1.0,\n 'AP_/people': 0.46231443}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'AP': 0.46231443,\n 'AP50': 0.9160241,\n 'AP75': 0.39535406,\n 'APs': 0.46239898,\n 'APm': -1.0,\n 'APl': -1.0,\n 'ARmax1': 0.16,\n 'ARmax10': 0.558,\n 'ARmax100': 0.601,\n 'ARs': 0.601,\n 'ARm': -1.0,\n 'ARl': -1.0,\n 'AP_/people': 0.46231443}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_u3eFxoBAiqE",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 18:49:48.360142: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2021-12-30 18:50:02.975026: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'resample_p7/PartitionedCall' has 1 outputs but the _output_shapes attribute specifies shapes for 3 outputs. Output shapes may be inaccurate.\n",
      "2021-12-30 18:50:10.515688: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2021-12-30 18:50:10.515715: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2021-12-30 18:50:10.515719: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:372] Ignored change_concat_input_ranges.\n",
      "2021-12-30 18:50:10.519686: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpop4lu1vk\n",
      "2021-12-30 18:50:10.595851: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2021-12-30 18:50:10.595884: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/tmpop4lu1vk\n",
      "2021-12-30 18:50:10.902450: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2021-12-30 18:50:12.759604: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /tmp/tmpop4lu1vk\n",
      "2021-12-30 18:50:13.441711: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 2922428 microseconds.\n",
      "2021-12-30 18:50:14.481192: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2021-12-30 18:50:15.769800: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1962] Estimated count of arithmetic ops: 1.752 G  ops, equivalently 0.876 G  MACs\n",
      "\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 0\n",
      "2021-12-30 18:51:35.721729: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1962] Estimated count of arithmetic ops: 1.752 G  ops, equivalently 0.876 G  MACs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.export(export_dir='.', tflite_filename='pplcnt.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jbl8z9_wBPlr",
    "outputId": "a0893077-28d8-4362-89f2-d5197222ac79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 66s 2s/step\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'AP': 0.44272077,\n 'AP50': 0.903442,\n 'AP75': 0.358373,\n 'APs': 0.44279137,\n 'APm': -1.0,\n 'APl': -1.0,\n 'ARmax1': 0.159,\n 'ARmax10': 0.521,\n 'ARmax100': 0.535,\n 'ARs': 0.535,\n 'ARm': -1.0,\n 'ARl': -1.0,\n 'AP_/people': 0.44272077}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_tflite('pplcnt.tflite', val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnqktl45PZRy"
   },
   "source": [
    "## Test the Android figurine detection model\n",
    "\n",
    "After training the model, let's test it with an image that the model hasn't seen before to get a sense of how good the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "9ZsLQtJ1AlW_",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#@title Load the trained TFLite model and define some visualization functions\n",
    "\n",
    "#@markdown This code comes from the TFLite Object Detection [Raspberry Pi sample](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi).\n",
    "\n",
    "import platform\n",
    "from typing import List, NamedTuple\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "\n",
    "Interpreter = tf.lite.Interpreter\n",
    "load_delegate = tf.lite.experimental.load_delegate\n",
    "\n",
    "# pylint: enable=g-import-not-at-top\n",
    "\n",
    "\n",
    "class ObjectDetectorOptions(NamedTuple):\n",
    "  \"\"\"A config to initialize an object detector.\"\"\"\n",
    "\n",
    "  enable_edgetpu: bool = False\n",
    "  \"\"\"Enable the model to run on EdgeTPU.\"\"\"\n",
    "\n",
    "  label_allow_list: List[str] = None\n",
    "  \"\"\"The optional allow list of labels.\"\"\"\n",
    "\n",
    "  label_deny_list: List[str] = None\n",
    "  \"\"\"The optional deny list of labels.\"\"\"\n",
    "\n",
    "  max_results: int = -1\n",
    "  \"\"\"The maximum number of top-scored detection results to return.\"\"\"\n",
    "\n",
    "  num_threads: int = 1\n",
    "  \"\"\"The number of CPU threads to be used.\"\"\"\n",
    "\n",
    "  score_threshold: float = 0.0\n",
    "  \"\"\"The score threshold of detection results to return.\"\"\"\n",
    "\n",
    "\n",
    "class Rect(NamedTuple):\n",
    "  \"\"\"A rectangle in 2D space.\"\"\"\n",
    "  left: float\n",
    "  top: float\n",
    "  right: float\n",
    "  bottom: float\n",
    "\n",
    "\n",
    "class Category(NamedTuple):\n",
    "  \"\"\"A result of a classification task.\"\"\"\n",
    "  label: str\n",
    "  score: float\n",
    "  index: int\n",
    "\n",
    "\n",
    "class Detection(NamedTuple):\n",
    "  \"\"\"A detected object as the result of an ObjectDetector.\"\"\"\n",
    "  bounding_box: Rect\n",
    "  categories: List[Category]\n",
    "\n",
    "\n",
    "def edgetpu_lib_name():\n",
    "  \"\"\"Returns the library name of EdgeTPU in the current platform.\"\"\"\n",
    "  return {\n",
    "      'Darwin': 'libedgetpu.1.dylib',\n",
    "      'Linux': 'libedgetpu.so.1',\n",
    "      'Windows': 'edgetpu.dll',\n",
    "  }.get(platform.system(), None)\n",
    "\n",
    "\n",
    "class ObjectDetector:\n",
    "  \"\"\"A wrapper class for a TFLite object detection model.\"\"\"\n",
    "\n",
    "  _OUTPUT_LOCATION_NAME = 'location'\n",
    "  _OUTPUT_CATEGORY_NAME = 'category'\n",
    "  _OUTPUT_SCORE_NAME = 'score'\n",
    "  _OUTPUT_NUMBER_NAME = 'number of detections'\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      model_path: str,\n",
    "      options: ObjectDetectorOptions = ObjectDetectorOptions()\n",
    "  ) -> None:\n",
    "    \"\"\"Initialize a TFLite object detection model.\n",
    "    Args:\n",
    "        model_path: Path to the TFLite model.\n",
    "        options: The config to initialize an object detector. (Optional)\n",
    "    Raises:\n",
    "        ValueError: If the TFLite model is invalid.\n",
    "        OSError: If the current OS isn't supported by EdgeTPU.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load metadata from model.\n",
    "    displayer = metadata.MetadataDisplayer.with_model_file(model_path)\n",
    "\n",
    "    # Save model metadata for preprocessing later.\n",
    "    model_metadata = json.loads(displayer.get_metadata_json())\n",
    "    process_units = model_metadata['subgraph_metadata'][0]['input_tensor_metadata'][0]['process_units']\n",
    "    mean = 0.0\n",
    "    std = 1.0\n",
    "    for option in process_units:\n",
    "      if option['options_type'] == 'NormalizationOptions':\n",
    "        mean = option['options']['mean'][0]\n",
    "        std = option['options']['std'][0]\n",
    "    self._mean = mean\n",
    "    self._std = std\n",
    "\n",
    "    # Load label list from metadata.\n",
    "    file_name = displayer.get_packed_associated_file_list()[0]\n",
    "    label_map_file = displayer.get_associated_file_buffer(file_name).decode()\n",
    "    label_list = list(filter(lambda x: len(x) > 0, label_map_file.splitlines()))\n",
    "    self._label_list = label_list\n",
    "\n",
    "    # Initialize TFLite model.\n",
    "    if options.enable_edgetpu:\n",
    "      if edgetpu_lib_name() is None:\n",
    "        raise OSError(\"The current OS isn't supported by Coral EdgeTPU.\")\n",
    "      interpreter = Interpreter(\n",
    "          model_path=model_path,\n",
    "          experimental_delegates=[load_delegate(edgetpu_lib_name())],\n",
    "          num_threads=options.num_threads)\n",
    "    else:\n",
    "      interpreter = Interpreter(\n",
    "          model_path=model_path, num_threads=options.num_threads)\n",
    "\n",
    "    interpreter.allocate_tensors()\n",
    "    input_detail = interpreter.get_input_details()[0]\n",
    "\n",
    "    # From TensorFlow 2.6, the order of the outputs become undefined.\n",
    "    # Therefore we need to sort the tensor indices of TFLite outputs and to know\n",
    "    # exactly the meaning of each output tensor. For example, if\n",
    "    # output indices are [601, 599, 598, 600], tensor names and indices aligned\n",
    "    # are:\n",
    "    #   - location: 598\n",
    "    #   - category: 599\n",
    "    #   - score: 600\n",
    "    #   - detection_count: 601\n",
    "    # because of the op's ports of TFLITE_DETECTION_POST_PROCESS\n",
    "    # (https://github.com/tensorflow/tensorflow/blob/a4fe268ea084e7d323133ed7b986e0ae259a2bc7/tensorflow/lite/kernels/detection_postprocess.cc#L47-L50).\n",
    "    sorted_output_indices = sorted(\n",
    "        [output['index'] for output in interpreter.get_output_details()])\n",
    "    self._output_indices = {\n",
    "        self._OUTPUT_LOCATION_NAME: sorted_output_indices[0],\n",
    "        self._OUTPUT_CATEGORY_NAME: sorted_output_indices[1],\n",
    "        self._OUTPUT_SCORE_NAME: sorted_output_indices[2],\n",
    "        self._OUTPUT_NUMBER_NAME: sorted_output_indices[3],\n",
    "    }\n",
    "\n",
    "    self._input_size = input_detail['shape'][2], input_detail['shape'][1]\n",
    "    self._is_quantized_input = input_detail['dtype'] == np.uint8\n",
    "    self._interpreter = interpreter\n",
    "    self._options = options\n",
    "\n",
    "  def detect(self, input_image: np.ndarray) -> List[Detection]:\n",
    "    \"\"\"Run detection on an input image.\n",
    "    Args:\n",
    "        input_image: A [height, width, 3] RGB image. Note that height and width\n",
    "          can be anything since the image will be immediately resized according\n",
    "          to the needs of the model within this function.\n",
    "    Returns:\n",
    "        A Person instance.\n",
    "    \"\"\"\n",
    "    image_height, image_width, _ = input_image.shape\n",
    "\n",
    "    input_tensor = self._preprocess(input_image)\n",
    "\n",
    "    self._set_input_tensor(input_tensor)\n",
    "    self._interpreter.invoke()\n",
    "\n",
    "    # Get all output details\n",
    "    boxes = self._get_output_tensor(self._OUTPUT_LOCATION_NAME)\n",
    "    classes = self._get_output_tensor(self._OUTPUT_CATEGORY_NAME)\n",
    "    scores = self._get_output_tensor(self._OUTPUT_SCORE_NAME)\n",
    "    count = int(self._get_output_tensor(self._OUTPUT_NUMBER_NAME))\n",
    "\n",
    "    return self._postprocess(boxes, classes, scores, count, image_width,\n",
    "                             image_height)\n",
    "\n",
    "  def _preprocess(self, input_image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Preprocess the input image as required by the TFLite model.\"\"\"\n",
    "\n",
    "    # Resize the input\n",
    "    input_tensor = cv2.resize(input_image, self._input_size)\n",
    "\n",
    "    # Normalize the input if it's a float model (aka. not quantized)\n",
    "    if not self._is_quantized_input:\n",
    "      input_tensor = (np.float32(input_tensor) - self._mean) / self._std\n",
    "\n",
    "    # Add batch dimension\n",
    "    input_tensor = np.expand_dims(input_tensor, axis=0)\n",
    "\n",
    "    return input_tensor\n",
    "\n",
    "  def _set_input_tensor(self, image):\n",
    "    \"\"\"Sets the input tensor.\"\"\"\n",
    "    tensor_index = self._interpreter.get_input_details()[0]['index']\n",
    "    input_tensor = self._interpreter.tensor(tensor_index)()[0]\n",
    "    input_tensor[:, :] = image\n",
    "\n",
    "  def _get_output_tensor(self, name):\n",
    "    \"\"\"Returns the output tensor at the given index.\"\"\"\n",
    "    output_index = self._output_indices[name]\n",
    "    tensor = np.squeeze(self._interpreter.get_tensor(output_index))\n",
    "    return tensor\n",
    "\n",
    "  def _postprocess(self, boxes: np.ndarray, classes: np.ndarray,\n",
    "                   scores: np.ndarray, count: int, image_width: int,\n",
    "                   image_height: int) -> List[Detection]:\n",
    "    \"\"\"Post-process the output of TFLite model into a list of Detection objects.\n",
    "    Args:\n",
    "        boxes: Bounding boxes of detected objects from the TFLite model.\n",
    "        classes: Class index of the detected objects from the TFLite model.\n",
    "        scores: Confidence scores of the detected objects from the TFLite model.\n",
    "        count: Number of detected objects from the TFLite model.\n",
    "        image_width: Width of the input image.\n",
    "        image_height: Height of the input image.\n",
    "    Returns:\n",
    "        A list of Detection objects detected by the TFLite model.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Parse the model output into a list of Detection entities.\n",
    "    for i in range(count):\n",
    "      if scores[i] >= self._options.score_threshold:\n",
    "        y_min, x_min, y_max, x_max = boxes[i]\n",
    "        bounding_box = Rect(\n",
    "            top=int(y_min * image_height),\n",
    "            left=int(x_min * image_width),\n",
    "            bottom=int(y_max * image_height),\n",
    "            right=int(x_max * image_width))\n",
    "        class_id = int(classes[i])\n",
    "        category = Category(\n",
    "            score=scores[i],\n",
    "            label=self._label_list[class_id],  # 0 is reserved for background\n",
    "            index=class_id)\n",
    "        result = Detection(bounding_box=bounding_box, categories=[category])\n",
    "        results.append(result)\n",
    "\n",
    "    # Sort detection results by score ascending\n",
    "    sorted_results = sorted(\n",
    "        results,\n",
    "        key=lambda detection: detection.categories[0].score,\n",
    "        reverse=True)\n",
    "\n",
    "    # Filter out detections in deny list\n",
    "    filtered_results = sorted_results\n",
    "    if self._options.label_deny_list is not None:\n",
    "      filtered_results = list(\n",
    "          filter(\n",
    "              lambda detection: detection.categories[0].label not in self.\n",
    "              _options.label_deny_list, filtered_results))\n",
    "\n",
    "    # Keep only detections in allow list\n",
    "    if self._options.label_allow_list is not None:\n",
    "      filtered_results = list(\n",
    "          filter(\n",
    "              lambda detection: detection.categories[0].label in self._options.\n",
    "              label_allow_list, filtered_results))\n",
    "\n",
    "    # Only return maximum of max_results detection.\n",
    "    if self._options.max_results > 0:\n",
    "      result_count = min(len(filtered_results), self._options.max_results)\n",
    "      filtered_results = filtered_results[:result_count]\n",
    "\n",
    "    return filtered_results\n",
    "\n",
    "\n",
    "_MARGIN = 10  # pixels\n",
    "_ROW_SIZE = 10  # pixels\n",
    "_FONT_SIZE = 1\n",
    "_FONT_THICKNESS = 1\n",
    "_TEXT_COLOR = (0, 0, 255)  # red\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    image: np.ndarray,\n",
    "    detections: List[Detection],\n",
    ") -> np.ndarray:\n",
    "  \"\"\"Draws bounding boxes on the input image and return it.\n",
    "  Args:\n",
    "    image: The input RGB image.\n",
    "    detections: The list of all \"Detection\" entities to be visualize.\n",
    "  Returns:\n",
    "    Image with bounding boxes.\n",
    "  \"\"\"\n",
    "  for detection in detections:\n",
    "    # Draw bounding_box\n",
    "    start_point = detection.bounding_box.left, detection.bounding_box.top\n",
    "    end_point = detection.bounding_box.right, detection.bounding_box.bottom\n",
    "    cv2.rectangle(image, start_point, end_point, _TEXT_COLOR, 3)\n",
    "\n",
    "    # Draw label and score\n",
    "    category = detection.categories[0]\n",
    "    class_name = category.label\n",
    "    probability = round(category.score, 2)\n",
    "    result_text = class_name + ' (' + str(probability) + ')'\n",
    "    text_location = (_MARGIN + detection.bounding_box.left,\n",
    "                     _MARGIN + _ROW_SIZE + detection.bounding_box.top)\n",
    "    cv2.putText(image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                _FONT_SIZE, _TEXT_COLOR, _FONT_THICKNESS)\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "1t1z2fKlAoB0",
    "outputId": "97e3d3d1-3168-4f6c-a754-ca1e6cf4f4a7",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File, 'android.tflite', does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_451042/622961601.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     19\u001B[0m       \u001B[0mscore_threshold\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mDETECTION_THRESHOLD\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m )\n\u001B[0;32m---> 21\u001B[0;31m \u001B[0mdetector\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mObjectDetector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mTFLITE_MODEL_PATH\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;31m# Run object detection estimation using the model.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_451042/3589236239.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, model_path, options)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m     \u001B[0;31m# Load metadata from model.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 92\u001B[0;31m     \u001B[0mdisplayer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmetadata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMetadataDisplayer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_model_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     93\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     94\u001B[0m     \u001B[0;31m# Save model metadata for preprocessing later.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow_lite_support/metadata/python/metadata.py\u001B[0m in \u001B[0;36mwith_model_file\u001B[0;34m(cls, model_file)\u001B[0m\n\u001B[1;32m    715\u001B[0m       \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mThe\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0mdoes\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mhave\u001B[0m \u001B[0mmetadata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    716\u001B[0m     \"\"\"\n\u001B[0;32m--> 717\u001B[0;31m     \u001B[0m_assert_file_exist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    718\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"rb\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    719\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_model_buffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow_lite_support/metadata/python/metadata.py\u001B[0m in \u001B[0;36m_assert_file_exist\u001B[0;34m(filename)\u001B[0m\n\u001B[1;32m    818\u001B[0m   \u001B[0;34m\"\"\"Checks if a file exists.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    819\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0m_exists_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 820\u001B[0;31m     \u001B[0;32mraise\u001B[0m \u001B[0mIOError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"File, '{0}', does not exist.\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    821\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    822\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: File, 'android.tflite', does not exist."
     ]
    }
   ],
   "source": [
    "#@title Run object detection and show the detection results\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "INPUT_IMAGE_URL = \"http://download.tensorflow.org/example_images/android_figurine.jpg\" #@param {type:\"string\"}\n",
    "DETECTION_THRESHOLD = 0.5 #@param {type:\"number\"}\n",
    "TFLITE_MODEL_PATH = \"android.tflite\" #@param {type:\"string\"}\n",
    "\n",
    "TEMP_FILE = '/tmp/image.png'\n",
    "\n",
    "!wget -q -O $TEMP_FILE $INPUT_IMAGE_URL\n",
    "image = Image.open(TEMP_FILE).convert('RGB')\n",
    "image.thumbnail((512, 512), Image.ANTIALIAS)\n",
    "image_np = np.asarray(image)\n",
    "\n",
    "# Load the TFLite model\n",
    "options = ObjectDetectorOptions(\n",
    "      num_threads=4,\n",
    "      score_threshold=DETECTION_THRESHOLD,\n",
    ")\n",
    "detector = ObjectDetector(model_path=TFLITE_MODEL_PATH, options=options)\n",
    "\n",
    "# Run object detection estimation using the model.\n",
    "detections = detector.detect(image_np)\n",
    "\n",
    "# Draw keypoints and edges on input image\n",
    "image_np = visualize(image_np, detections)\n",
    "\n",
    "# Show the detection result\n",
    "Image.fromarray(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWP3fEPaGNvd"
   },
   "source": [
    "## Compile the model for EdgeTPU\n",
    "\n",
    "Finally, we'll compile the model using `edgetpu_compiler` so that the model can run on [Google Coral EdgeTPU](https://coral.ai/).\n",
    "\n",
    "We start with installing the EdgeTPU compiler on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kK6AN1xVAsCb",
    "outputId": "0a199dc3-a4df-4b4b-ef2b-b9d88b14cb53",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install edgetpu-compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIGSdzXkEzrj"
   },
   "source": [
    "**Note:** When training the model using a custom dataset, beware that if your dataset includes more than 20 classes, you'll probably have slower inference speeds compared to if you have fewer classes. This is due to an aspect of the EfficientDet architecture in which a certain layer cannot compile for the Edge TPU when it carries more than 20 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzF6u0FZTAjF"
   },
   "source": [
    "Before compiling the `.tflite` file for the Edge TPU, it's important to consider whether your model will fit into the Edge TPU memory. \n",
    "\n",
    "The Edge TPU has approximately 8 MB of SRAM for [caching model paramaters](https://coral.ai/docs/edgetpu/compiler/#parameter-data-caching), so any model close to or over 8 MB will not fit onto the Edge TPU memory. That means the inference times are longer, because some model parameters must be fetched from the host system memory.\n",
    "\n",
    "One way to elimiate the extra latency is to use [model pipelining](https://coral.ai/docs/edgetpu/pipeline/), which splits the model into segments that can run on separate Edge TPUs in series. This can significantly reduce the latency for big models.\n",
    "\n",
    "The following table provides recommendations for the number of Edge TPUs to use with each EfficientDet-Lite model.\n",
    "\n",
    "| Model architecture | Minimum TPUs | Recommended TPUs\n",
    "|--------------------|-------|-------|\n",
    "| EfficientDet-Lite0 | 1     | 1     |\n",
    "| EfficientDet-Lite1 | 1     | 1     |\n",
    "| EfficientDet-Lite2 | 1     | 2     |\n",
    "| EfficientDet-Lite3 | 2     | 2     |\n",
    "| EfficientDet-Lite4 | 2     | 3     |\n",
    "\n",
    "If you need extra Edge TPUs for your model, then update `NUMBER_OF_TPUS` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JyptUjakAwzz",
    "outputId": "7a3dad3b-eeb4-45e1-fb68-f0436ac5a933",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_TPUS = 1\n",
    "\n",
    "!edgetpu_compiler android.tflite --num_segments=$NUMBER_OF_TPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJYXucYWTGqZ"
   },
   "source": [
    "Finally, we'll copy the metadata, including the label file, from the original TensorFlow Lite model to the EdgeTPU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LY1WrgMJBFd",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "populator_dst = metadata.MetadataPopulator.with_model_file('android_edgetpu.tflite')\n",
    "\n",
    "with open('android.tflite', 'rb') as f:\n",
    "  populator_dst.load_metadata_and_associated_files(f.read())\n",
    "\n",
    "populator_dst.populate()\n",
    "updated_model_buf = populator_dst.get_model_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "VdRihInCJ3ie",
    "outputId": "f2c37370-6ec3-41e6-903d-22e49e2f7dea",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Download the TFLite model compiled for EdgeTPU to your local computer.\n",
    "from google.colab import files\n",
    "files.download('android_edgetpu.tflite')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model Maker Object Detection for Android Figurine",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}